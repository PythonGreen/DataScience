---
title: 'A1 - PreProcesado de Datos'
author: "Pablo A. Delgado"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  html_document:
    highlight: default
    theme: cosmo
    toc: yes
    toc_depth: 4
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---    
    

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción
En esta actividad realizaremos el pre procesado de un fichero de datos que contiene el estilo de juego del videojuego de consola FIFA 2017, así como estadísticas reales de los jugadores de futbol. El conjunto de datos contiene más de 17500 registros y 53 variables.

Las principales variables que se usarán en esta actividad son:

    •	Name (Nombre del jugador)
    •	Nationality (Nacionalidad del jugador)
    •	Club_Joining (Fecha en la que empezó en el club)
    •	Contract_Expire (Año finalización del contrato)
    •	Rating (Valoración global del jugador, entre 0 y 100)
    •	Height (Altura)
    •	Weight (Peso)
    •	Preffered_Foot (Pie preferido)
    •	Birth_Date (Fecha de nacimiento)
    •	Age (Edad)
    •	Work_Rate (valoración cualitativa en términos de ataque-defensa)
    
La descripción de los atributos se puede consultar en https://www.fifplay.com/encyclopedia. La descripción de las abreviaturas de la posición del jugador en el campo se puede consultar en 
https://www.dtgre.com/2016/10/fifa-17-position-abbreviations-acronyms.html.

El objetivo de esta actividad es preparar el fichero para su posterior análisis. Para ello, se examinará el fichero para detectar y corregir posibles errores, inconsistencias y valores perdidos. Además, se presentará una breve estadística descriptiva y se hará un análisis de componentes principales (PCA) con algunas variables cuantitativas.

**La actividad consta de tres partes diferenciades**:

  1. En la primera parte (secciones 2, 3, 4 y 5), se realiza verificaciones y normalización de algunes variables, siguiendo los criterios que se especifican más adelante.
  2. En la segunda parte (secciones 6 y 7), se tratan los valores atípicos y los valores perdidos.
  3. En la tercera parte (secciones 8 y 9), se calculan algunas métricas de tendencia central y dispersión, que sería el primer paso del análisis descriptivo y, por último, se realiza un análisis de componentes principales (ACP) con algunas variables cuantitativas.

**Criterios de verificación y de normalización de las variables**:

A continuación se muestran los criterios con los que deben limpiarse los datos del conjunto:

  1. En los datos numéricos, el símbolo de separador decimal es el punto y no la coma.
  2. Verificar si hay registros duplicados con el valor ID. En caso de duplicación, seleccionar el registro con menor numero de NAs en las variables.
  3. Las variables Name y Nationality no han de tener espacios en blanco antes o después de su valor. El valor para estas variables han de ser mayúsculas en la primera letra de cada palabra, tal como "Lionel Messi".
  4. La variable Height se ha de expresar en cm con 3 dígitos sin decimales. Para facilitar la lectura y tratamiento del fichero deben ser numéricas, por lo tanto se debe quitar el símbolo de cm.
  5. La variable Weight se ha de expresar en kg con 2 dígitos sin decimales. Si hay decimales, se ha de truncar el valor. Para facilitar la lectura y tratamiento del fichero deben ser numéricas, por lo tanto se debe quitar el símbolo de kg.
  6. Verificar que la variable Club_Joining está en el rango de los años 1990 a 2017. En caso de haber algún registro que no compla la condición, indicar el número de registro, Name y Club_joining.
  7. Verificar que el año de expiración del contracto (Contract_Expiry) no es inferior al año de inicio del contracto(Club_Joining). En caso de haber algún registro que no cumple la condición, indicar el número de registro, Name, Club_Joining y Contract_Expiry.
  8. Verificar que la edad (Age) en la fecha 1/1/2017 corresponde a la calculada con la fecha de nacimiento (Birth_Date). En caso de haber algún registro que no cumpla la condición, modificar la edad en función del valor obtenido con la fecha de nacimiento.
  9. Verificar que la variable Rating esté entre 0 y 100.
  10. La variable Preffered_Foot ha de tener los valores Left y Right que corresponde a los valores actuales de 1 y 2, respectivamente.
  11. La variable Work_Rate se basa en la combinación de dos de estas tres categorías: Low, Medium y High. Verificar que se cumple y en caso contrario, hay que corregirlo. Puedes encontrar los nombres de las categorías cortado con tres letras.


# 1. Carga del archivo

Se debe abrir el archivo de datos y examinar el tipo de datos con los que R ha interpretado cada variable.
Examinar también los valores resumen de cada tipo de variable.


Como primer paso definimos que librerias estaremos usando. Para ellos generamos un vector con todas las posibles librerias que necesitaremos, instalamos las que no tengamos, para finalmente mediante el uso de lapply cargarlas.

```{r}
packages <- c("corrplot", "stringr","ggplot2", "stats", "tools", "eeptools", "gridExtra", "psych","factoextra")
new <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new)) install.packages(new)
foo=lapply(packages, require, character.only=TRUE)
```

Dado que el input de datos es un archivo csv, y en una inspeccion visual manual hemos visto que posee como separador de columna la coma, usaremos la funcion `read.csv()` para cargar los datos en un dataframe. Vale mencionar que en esta "rapida" inspeccion visual tambien hemos visto que valores como el peso y la altura poseen como separador decimal la coma y en otros casos el punto. Pero dado que estan entre comillas (ademas de estar en unidades distintas), con read.csv no tendremos problemas, nos ocuperamos de normalizar todos estos valores posteriormente.
Tambien hemos podido constatar que el csv cuenta con 54 columnas y 17590 lineas, la primera corresponde al header y la ultima una linea en blanco, siendo asi 17588 lineas con datos.

Dicho esto carguemos el archivo y hagamos una verificacion rapida de las primeras y ultimas 5 filas del dataframe.

```{r read}
fifa2017 <- read.csv("fifa_raw.csv", stringsAsFactors = FALSE, header=TRUE)
head(fifa2017)
tail(fifa2017)
```

Veamos la cantidad de columnas, filas y como quedaron los tipos de datos en el dataframe para entender luego como hacer el trabajo de preprocesado de los mismos.
```{r}
str(fifa2017) 
nrow(fifa2017)
ncol(fifa2017)

```
En principio como vemos, 17588 obs. of  54 variables, la cantidad de filas y columnas en el dataframe coinciden con la inspecion visual que hemos hecho sobre el archivo csv.

En cuanto a los tipos de datos es evidente que tendremos que normalizar varias variables, cambiar tipos de datos, completar valores vacios, etc.. como puede verse facilmente con por ej las variables: Height, Weight, National_Kit, etc...

Veamos cuales son las estadisticas basicas tal como tenemos hasta ahora el dataframe sin ningun trabajo de procesado por el momento:
```{r}
summary(fifa2017)
```

Como hemos visto arriba, muchas variables cuantitivas se cargaron por default como character, ademas de como hemos dicho antes, incluso como character tampoco estan en el mismo formato por lo que deberemos normalizar las unidades de varias variables al convertirlas de character a numeric o integer (cuantitativas continuas o cuantitativas discretas). O incluso tambien normalizar si fuera la conversion a cualitativas si correspondiense.

Tambien puede evidenciarse arriba que tenemos incluso casos de variables categoricas ocultas como cuantitativas, como por ej la variable Preffered_Foot, que toma valores 1 o 2. Fue importada por default por la funcion read.csv como integer, siendo posible considerarla como Categorica Nominal.


# 2. Verificar duplicación de registros

Verifiquemos si hay registros duplicados a partir del valor ID. En caso que existan duplicados, seleccionaremos el registro con menor numero de NAs en las variables.


```{r}
length(fifa2017$ID)
length(unique(fifa2017$ID))
```
Como vemos aqui, la cantidad de observaciones son coincidentes con y sin aplicar la funcion unique, por lo que podemos concluir que no existen duplicados por el campo ID.


# 3. Normalización de los datos cuantitativos

Inspeccionar los valores de los datos cuantitativos y realizar las normalizaciones oportunas siguiendo los
criterios especificados anteriormente. Estas normalizaciones tienen como objetivo uniformizar los formatos. Si hay valores perdidos o valores extremos, se tratarán más adelante.

Al realizar estas normalizaciones, se debe demostrar que la normalización sobre cada variable ha dado el
resultado esperado. Por lo tanto, se recomienda mostrar un fragmento del archivo de datos resultante. Para
evitar mostrar todo el conjunto de datos, se puede mostrar una parte del mismo, con las funciones head y/o
tail.

Puntos a tener en cuenta:

    - En los datos numéricos, el símbolo de separador decimal es el punto y no la coma.
    - Verificar que la variable Rating esté entre 0 y 100.
    - La variable Height se ha de expresar en cm con 3 dígitos sin decimales. Para facilitar la lectura y tratamiento del fichero deben ser numéricas, por lo tanto se debe quitar el símbolo de cm.
    - La variable Weight se ha de expresar en kg, sin decimales. Si hay decimales, se ha de truncar el valor. Para facilitar la lectura y tratamiento del fichero deben ser numéricas, por lo tanto se debe quitar el símbolo de kg.
  
## 3.1. Rating

Primero verificamos jugadores que tenga un rating fuera del rango de 0 a 90:

```{r}
fifa2017[!(fifa2017$Rating >= 0 & fifa2017$Rating <=90), c("ID","Name", "Rating")]
```
Por lo que vemos arriba justamente son los cracks los que tienen valores por encima de 90, pero dicho esto y lo que estamos visualizando, no existen jugadores con un rating por encima de 100, ni mucho menos por debajo de 0. Reconfirmemoslo con la siguiente sentencia:

```{r}
fifa2017[!(fifa2017$Rating >= 0 & fifa2017$Rating <=100), c("ID","Name", "Rating")]
```

## 3.2. Height

Como sabemos y vemos aqui debemos llevar a la misma unidad la variable altura, a cm y sin decimales:

```{r}
fifa2017$Height[1:20]
```
Como primer paso verifiquemos que la cantidad de missing values, y la cantidad de observaciones con la unidad cm y m coincida con el total de observaciones: 17588

```{r}
length(grep(' cm', fifa2017$Height)) + 
length(grep(' m', fifa2017$Height)) +
table(is.na(fifa2017$Height))["TRUE"]
```
Dicho, esto llevemos ahora todos las observaciones en metros directamente a centimetros aqui:

```{r}
fifa2017$Height = str_replace_all(fifa2017$Height, "[cm ,]", "")
fifa2017$Height = as.integer(str_pad(fifa2017$Height, 3, side = c("right"), pad = "0"))
```

Verifiquemos que el tipo de dato sea integer y grafiquemos un sencillo grafico de barras para visualizar si los datos de altura quedaron ok

```{r}
class(fifa2017$Height)
#ggplot(data=as.data.frame(fifa2017$Height), aes(x=fifa2017$Height))+geom_bar()
barplot(table(fifa2017$Height))
```


## 3.3. Weight


La variable Weight se ha de expresar en kg, sin decimales. Si hay decimales, se ha de truncar el valor. Para facilitar la lectura y tratamiento del fichero deben ser numéricas, por lo tanto se debe quitar el símbolo de kg.

Si observamos este sampleo de datos, vemos que para separar los decimales cuando la unidad es kg se usan indistintamente comas o puntos, mientras que cuando tenemos solo gramos no se usa ningun tipo de separador.

```{r}
head(fifa2017$Weight,20)
```

Hagamos un check mas para el caso de gramos:

```{r}
length(grep('gr', fifa2017$Weight))
```

Dado que son pocos registros, verifiquemos que no exista ningun tipo de separador.
```{r}
fifa2017[grep('gr', fifa2017$Weight),]$Weight
```


Por lo tanto, para obtener lo solicitado haremos:

    1. Dado que el símbolo de separador decimal es el punto y no la coma, cambiaremos todos las comas por puntos
    2. Identificamos los indices con el valor gr
    3. Identificamos los indices con el valor kg
    4. Para los elementos del grupo gr, les quitamos el string gr, convertimos a numerico, dividimos por 1000 para convertir a kilogramos y luego truncamos decimales.
    5. Para los elementos del grupo kg, les quitamos el string kg, convertimos a numerico y luego truncamos decimales

Veamos y ejecutemos ahora el codigo para realizar esos pasos:
```{r}
# Dado que el símbolo de separador decimal es el punto y no la coma, realizamos la conversion:
fifa2017$Weight = str_replace_all(fifa2017$Weight, ",", ".")

# Detectamos los indices con unidad kg y los correspondientes a gr
indexes_gr = grep('gr', fifa2017$Weight)
indexes_kg = grep('kg', fifa2017$Weight)


# Chequeamos cantidades
length(indexes_gr)
length(indexes_kg)


# Hacemos una previsualizacion de ambos
head(fifa2017[indexes_gr,]$Weight,10)
head(fifa2017[indexes_kg,]$Weight,10)

# Dividimos por 1000 los valores correspondientes a gr y truncamos los decimales
fifa2017[indexes_gr,]$Weight = trunc(as.numeric(str_replace_all(fifa2017[indexes_gr,]$Weight, "[ gr]", ""))/1000)

# Convertimos los valores correspondientes a kg a numerico y truncamos los decimales
fifa2017[indexes_kg,]$Weight = trunc(as.numeric(str_replace_all(fifa2017[indexes_kg,]$Weight, "[ kg]", "")))
fifa2017$Weight = as.integer(fifa2017$Weight)

# previsualicemos algunos valores
head(fifa2017$Weight,20)
```



# 4. Normalización de los datos cualitativos

Inspeccionar los valores de los datos cualitativos y realizar las transformaciones oportunas, siguiendo los criterios especificados. Al igual que en el apartado anterior, mostrar el resultado sobre un fragmento del conjunto de datos.


## 4.1. Name y Nationality

Las variables Name y Nationality no han de tener espacios en blanco antes o después de su valor. El valor para estas variables han de ser mayúsculas en la primera letra de cada palabra, tal como "Lionel Messi".

Como vemos en este sampleo, puede verificarse algunos de los puntos comentado en el parrafo anterior, incluso para los paises en algunos casos esta todo en mayuscula, por ej: ENGLAND, por lo que debera convertirse a England, segun lo definido.

```{r}
head(fifa2017$Name,20)
tail(fifa2017$Nationality,20)
```
Ejecutemos el codigo para realizar la transformacion:
```{r}
# quitamos espacios en los extremos
fifa2017$Name=str_trim(fifa2017$Name, side = c("both"))
fifa2017$Nationality=str_trim(fifa2017$Nationality, side = c("both"))

# convertimos todo a minusculas, para igualar primero todas las palabras, por ej England e ENGLAND, a england
fifa2017$Name = tolower(fifa2017$Name)
fifa2017$Nationality = tolower(fifa2017$Nationality)


# para finalmente convertir a mayuscula cada palabra de cada cada observacion
fifa2017$Name = toTitleCase(fifa2017$Name)
fifa2017$Nationality = toTitleCase(fifa2017$Nationality)
```

Volvamos a verificar que se haya realizado el ajuste correctamente:
```{r}
head(fifa2017$Name,20)
tail(fifa2017$Nationality,20)
```
## 4.2. Preffered_Foot

La variable Preffered_Foot ha de tener los valores Left y Right que corresponde a los valores actuales de 1 y 2, respectivamente.

En primer instancia verifiquemos los valores posibles de esta variable:

```{r}
table(fifa2017$Preffered_Foot)
```

Veamos el codigo para realizar la transformacion:
```{r}
fifa2017[(fifa2017$Preffered_Foot==1),]$Preffered_Foot = 'Left'
fifa2017[(fifa2017$Preffered_Foot==2),]$Preffered_Foot = 'Right'
```

Verifiquemos el resultado de la modificacion:

```{r}
table(fifa2017$Preffered_Foot)
```


## 4.3. Work_Rate

La variable Work_Rate se basa en la combinación de dos de estas tres categorías: Low, Medium y High. Verificar que se cumple y en caso contrario, hay que corregirlo. Puedes encontrar los nombres de las categorías cortado con tres letras.

En primer instancia verifiquemos los valores posibles de esta variable:

```{r}
table(fifa2017$Work_Rate)
sum(table(fifa2017$Work_Rate))
```
Por lo que vemos arriba, deberemos por ej renombrar casos como:
Hig a High 
Med a Medium 

```{r}

fifa2017$Work_Rate = str_replace(fifa2017$Work_Rate, "Hig /", "High /")
fifa2017$Work_Rate = str_replace(fifa2017$Work_Rate, "/ Hig$", "/ High")
fifa2017$Work_Rate = str_replace(fifa2017$Work_Rate, "Med /", "Medium /")
fifa2017$Work_Rate = str_replace(fifa2017$Work_Rate, "/ Med$", "/ Medium")

```

Vemos ahora como se han reagrupado correctamente las observaciones
```{r}
table(fifa2017$Work_Rate)
```
y los valores totales siguen cuadrando:
```{r}
sum(table(fifa2017$Work_Rate))
```

# 5. Posibles inconsistencias y variables tipo fecha

Verificar si existen inconsistencias entre algunas variables. Por otra parte, algunas de las variables es necesario indicar que son de tipo fecha (en r, tipo Date) para luego hacer las transformaciones adecuadas. Observar que la configuración del tipo fecha es mes/día/año. Al igual que en el apartado anterior, muestre el resultado sobre un fragmento del conjunto de datos.


## 5.1. Club_Joining

Verificar que la variable Club_Joining está en el rango de los años 1990 a 2017. En caso de haber algún registro que no cumpla la condición, indicar el número de registro, Name y Club_joining.

Para esto primero convertimos el campo a date, ya que como vemos es del tipo character:
```{r}
class(fifa2017$Club_Joining)
typeof(fifa2017$Club_Joining)
```
Luego verificamos si hay algun formato incorrecto al intentar convertir a date:
```{r}
indexs_date_ok = which(!is.na(as.Date(fifa2017$Club_Joining, "%m/%d/%Y")))
indexs_date_nook =  which(is.na(as.Date(fifa2017$Club_Joining, "%m/%d/%Y")))
length(indexs_date_ok)
length(indexs_date_nook)
```
Como vemos no hay ninguno, solo 1 registro vacio.
```{r}
fifa2017[indexs_date_nook,]$Club_Joining
```

Prosigamos:

```{r}
fifa2017$Club_Joining = as.Date(fifa2017$Club_Joining, "%m/%d/%Y")
head(fifa2017$Club_Joining)
```

Para finalmente ver los jugadores donde su Club_Joining no está en el rango de los años 1990 a 2017, mostrando su ID, Nombre y Fecha.

```{r}
# Previsualizamos la cantidad de jugadores distribuidos por año
table(format(fifa2017$Club_Joining, format = "%Y"))

# Mostramos los que no cumplen la condicion requerida
cols = c("ID", "Name", "Club_Joining")
fifa2017[!(fifa2017$Club_Joining >= as.Date('1990-01-01') & fifa2017$Club_Joining < as.Date('2018-01-01') ), cols ]
```

Como finalmente vemos todos estan en el rango definido, ya que estamos considerando del 1 de enero de 1990 al 31 de diciembre de 2017.

## 5.2. Contract_Expiry >= Club_Joining?
Verificar que el año de expiración del contracto (Contract_Expiry) no es inferior al año de inicio del contracto(Club_Joining). En caso de haber algún registro que no cumple la condición, indicar el número de registro, Name, Club_Joining y Contract_Expiry.


Realizamos primero un sampleo de las observaciones por Contract_Expiry.
```{r}
table(fifa2017$Contract_Expiry)
```

Para luego mostrar lo solicitado
```{r}
cols = c("ID", "Name", "Club_Joining", "Contract_Expiry")
years_club_Joining = format(fifa2017$Club_Joining, format = "%Y")
fifa2017[!(fifa2017$Contract_Expiry >= years_club_Joining ), cols ]
```

Como puede verse no hay registros donde el año de expiracion sea anterior al año de incorporacion al equipo. 

Si es verdad que hay casos de jugadores que se unen a un equipo y su contrato expira ese mismo año, como por ejemplo estos casos:
```{r}
nrow(fifa2017[fifa2017$Contract_Expiry == years_club_Joining,])
head(fifa2017[fifa2017$Contract_Expiry == years_club_Joining, cols])
```


## 5.3. Revisar si la edad corresponde a la fecha de nacimiento

Verificar que la edad (Age) en la fecha 1/1/2017 corresponde a la calculada con la fecha de nacimiento (Birth_Date). En caso de haber algún registro que no cumpla la condición, modificar la edad en función del valor obtenido con la fecha de nacimiento.

En primer instancia verifiquemos el campo birth_date y age

```{r}
table(fifa2017$Age)
class(fifa2017$Birth_Date)
head(fifa2017$Birth_Date)
```



Ahora asumiendo que se quiere calcular las edades al 1 de enero de 2017, vamos a verificar si el campo edad que nos viene informado en el csv es igual o no a la diferencia entre esta fecha (1/1/2017) y la fecha de nacimiento de cada jugador:
```{r}
# primero convertimos a fecha el birth_date
fifa2017$Birth_Date = as.Date(fifa2017$Birth_Date, "%m/%d/%Y")

calculated_ages =  trunc(age_calc(fifa2017$Birth_Date, enddate = as.Date('2017-01-01') , units = "years", precise = TRUE))

nrow(fifa2017[fifa2017$Age != calculated_ages, ])
```
Como vemos hay 5561 casos donde la edad no es la misma, ajustemos esos valores de edad al 1/1/2017

```{r}
index_ages_to_fix = which(fifa2017$Age != calculated_ages)
fifa2017[index_ages_to_fix,]$Age = calculated_ages[index_ages_to_fix]
```

Volvamos a verificar que ahora ya no existan  diferencias:
```{r}
nrow(fifa2017[fifa2017$Age != calculated_ages, ])
```

Hagamos un pequeño sampleo del resultado de ambos grupos, de los ok, como de los que se corrigieron
```{r}
cols = c("Name", "Age", "Birth_Date")
head(fifa2017[-index_ages_to_fix,cols],20)
head(fifa2017[index_ages_to_fix,cols],20)
```





# 6. Valores atípicos
Revisar si hay valores atípicos en la variable Height y Weight. Si es así, y se trata de un valor anormalmente alto o bajo, se recomienda sustituir el valor por “NA”.


Como vemos aqui no hay valores centinelas extremos, ya que para la altura tenemos un valor maximo de 207 cm y minimo de 155 cm, que pueden ser valores normales para jugadores de futbol, de hecho la media es de 181cms.
Caso similar tenemos para el peso con valores maximos de 110kg y minimos de 48kg.
En ambos casos tenemos 3 NA.

```{r}
summary(fifa2017$Height)
summary(fifa2017$Weight)
```
Dicho esto creemos que no hace falta reasignar a ninguno de esos valores extremos el valor NA, para asi no perder capacidad de analisis de la informacion.

Pero que pasa si graficamos estos valores en barplots y boxplots?

```{r}

hbp=ggplot(fifa2017,aes(x=Height,fill=Height))+geom_boxplot(fill="lightblue")+theme_bw()
hb=ggplot(fifa2017,aes(x=Height,fill=Height))+geom_bar(fill="lightblue")+theme_bw()

wbp=ggplot(fifa2017,aes(x=Weight,fill=Weight))+geom_boxplot(fill="lightblue")+theme_bw()
wb=ggplot(fifa2017,aes(x=Weight,fill=Weight))+geom_bar(fill="lightblue")+theme_bw()

grid.arrange(grobs = list(hbp, wbp, hb, wb) , nrow = 2, ncol = 2)

```

Al verlo graficamente, notamos que los valores extremos que nos dio el summary en principio son valores "normales" al ojo humano pero que se "alejan" de la tendencia central de nuestra muestra de datos.

Con esta comando podemos de que valores se tratan estos "outliers" de la muestra:
```{r}
# Valores atipicos de altura
sort(boxplot.stats(fifa2017$Height)$out)
# Valores atipicos en el peso
sort(boxplot.stats(fifa2017$Weight)$out)
```

Podriamos optar aqui por convertirlos a NA por ser "outliers" en esta muestra, pero dado que consideramos que no son valores "anormalmente" altos o bajos en princio los dejaremos, ya que es posible que existan jugadores de 160cm como de 207cm o de 50kg como de 110kg. Es verdad que es poco probable encontrar a hombres de menos de 55kg, pero puede darse, no es un valor "incorrecto".


Ademas tal como se menciona en los recursos de la asignatura y es nuestro caso: "En algunas ocasiones el valor atipico es cambiado por «no disponible» (NA) y, en otras ocasiones, se queda tal como está ya que puede ser un valor real pero muy poco frecuente."

Veamos un ejemplo de jugador de bajo peso:

```{r}
cols = c("Name", "Age", "Weight", "Height", "Nationality")
fifa2017[fifa2017$Weight == 48,cols]
```

En los puntos siguientes de esta practica igualmente podremos validar al momento de realizar el EDA y PCA que sucede con y sin estos valores no tan "tipicos" (si es que vemos que estos valores desvirtuan el analisis) simplemente convirtiendo en NA el peso y la altura de estas **pocas** observaciones que desplegamos aqui:

```{r}
# Pesos "atipicos"
fifa2017[fifa2017$Weight %in% boxplot.stats(fifa2017$Weight)$out, cols]

# Alturas "atipicas"
fifa2017[fifa2017$Height %in% boxplot.stats(fifa2017$Height)$out, cols]
```


# 7. Imputación de valores

Buscar en las variables Weight y Height donde haya valores perdidos (NA) y realice una imputación de
valores. Para realizar una imputación de valores necesita hacer una regresión lineal que tenga como variable a predecir
la variable con la NAs. Esto significa que hay que realizar dos modelos de regresión lineal, uno para predecir
los valores NAs en Weight a partir de la variable Height. El otro es precisamente al revés, predecir los valores
NAs en Height a partir de Weight. La función para hacer regresión lineal es lm().

Muestre el resultado de la imputación y de la variable explicativa en aquellos casos donde había un NA.

Identifiquemos primero los registros con NA de ambas variables
```{r}
na_height = which(is.na(fifa2017$Height))
na_weight = which(is.na(fifa2017$Weight))
cols = c("Name", "Age", "Weight", "Height", "Nationality")
fifa2017[na_height, cols]
fifa2017[na_weight, cols]
```
Creemos y visualicemos los modelos
```{r}
height_model = lm(fifa2017$Height ~ fifa2017$Weight)
weight_model = lm(fifa2017$Weight ~ fifa2017$Height)
height_model
weight_model
```

Imputemos los valores obtenidos por la prediccion y visualicemos los resultados:
```{r}
fifa2017[na_height, "Height"] = as.integer(predict(height_model, newdata = fifa2017[na_height,])[na_height])
fifa2017[na_weight, "Weight"] = as.integer(predict(weight_model, newdata = fifa2017[na_weight,])[na_weight])

# Asi fue la prediccion para las alturas
fifa2017[na_height, cols]
# Y asi el resultado de la prediccion de los pesos
fifa2017[na_weight, cols] 
```



# 8. Estudio descriptivo de las variables cuantitativas.

Realice un breve estudio descriptivo de las variables cuantitativas una vez depuradas. Hay que crear una
tabla con medidas de tendencia central y de dispersión, tanto robustas como no robustas. Haga un breve
comentario sobre los resultados obtenidos entre estos tipos de medidas en todas las variables.

Luego de haber normalizado la mayoria de las variables veamos como quedo nuestro dataframe:

```{r}
str(fifa2017)
```

Quedemosnos con algunas variables cuantitivas para realizar un estudio descriptivo
```{r}
cols = c("Rating", "Height", "Weight", "Age", "Ball_Control", "Dribbling", "Freekick_Accuracy", "Shot_Power")
```

Tal como se comenta en los recursos de aprendizaje se puede hacer uso del paquete psych para utilizar estimadores robustos a la hora de describir las variables de una muestra.

Pero antes de continuar realizamos un breve repaso de los estimadores mas comunes:
  
  **Estimadores de Tendencia Central:**
    
    - El estimador de tendencia central clásico es la media aritmética. Tiene muy buenas propiedades; es un estimador insesgado, 
    pero su estimación está muy influida por los valores atípicos.
    
    - La mediana es uno de los estimadores robustos más conocidos para medir la tendencia central de los datos.
    
    - Otro estimador robusto para medir la tendencia central es la media recortada (trimmed mean).
    
    - Una variante de la media recortada es la media winsorizada (winsorized mean).
    
  **Estimadores de Dispersion:**
  
    - Para medir la dispersión de los datos el estimador más habitual es la desviación estándar, que no es un estimador robusto.
    
    - El rango intercuartílico (RIC) y la desviación absoluta respecto de la mediana (DAM) son alternativas robustas a la desviación estándar


Ahora si veamos estos estimadores en numeros con la ayuda de psych

```{r}
describe(fifa2017[cols], na.rm = TRUE,  IQR = TRUE, trim=.05 )
```

Si nos focalizamos en los estimadores de tendencia central, puede verse para la mayoria de las variables que la media recortada de un 5% se acerca mas a la mediana, estimador robusto, que la media aritmetica.

Mientras que la DAM y el RIC, en algunos casos mejora los valores mostrados por la desviacion standar, como ser el caso de Age.

# 9. Análisis de Componentes Principales (PCA)

Realizar el Análisis de Componentes Principales sobre las variables “Rating”, “Height”, “Weight” y “Age”.
Representar el gráfico biplot de dos dimensiones. Hacer un breve comentario indicando el porcentaje de
variabilidad explicada en cada componente principal, la variabilidad explicada en las dos primeras dimensiones y qué variable original está más asociada a cada una de las dos primeras componentes principales. Interpretar.

Por ultimo, ¿hay algún punto más fuera de la nube de puntos?, si es así puedes mostrar los valores de las
variables originales e indicar que tiene de especial este punto.


Ante todo verificamos que exista correlacion entre algunas de las variables para que la aplicacion de PCA tenga sentido
```{r}
cols = c("Rating", "Height", "Weight", "Age")
cor(fifa2017[cols])
corrplot(cor(fifa2017[cols]), type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

Al menos tenemos una alta correlacion entre el peso y la altura, como era de esperar. Y poca entre el rating y la edad. Y casi nula entre estos 2 grupos de variables.

Veamos la proporcion de varianza explicada con cada variable aplicando PCA mediante el uso de la matriz de las correlaciones. Utilizamos este metodo dado que las variables de analisis tienen magnitudes distintas, cm para la altura, kg para el peso, y sin unidades para la edad y el rating.

De hecho si hacemos la comparacion entre ambos metodos vemos que si bien con ambos necesitamos 3 componentes para explicar mas del 90% de la varianza, es con con la matriz de correlaciones cuando obtenemos un minimo mejor porcentaje (94.31% vs 93.83)
```{r}
PCA.cor <- prcomp(x=fifa2017[cols], center = TRUE, scale.= TRUE)
PCA.cov <- prcomp(x=fifa2017[cols], center = TRUE, scale.= FALSE)
summary(PCA.cor)
summary(PCA.cov)
```
Vale aclarar que si usaramos por ejemplo solo dos componentes, con la matriz de covarianzas obtenemos una mayor varianza explicada a pesar que las magnitudes de las variables sean diferentes, 84.25% vs  80.72%. Por lo que si nos fuera suficiente solo un 80% minimo de la varianza podriamos optar por usar las dos primeras componenetes usando covarianzas. 

Tal como se describe en los recursos de aprendizaje, la varianza de cada componente principal es el valor propio y se calcula elevando al cuadrado PCA.cov$sdev (o PCA.cor$sdev para correlaciones). Para obtener el porcentaje de la varianza explicada en cada componente principal hay que dividir cada valor propio entre la suma de todos los valores propios, y da un resultado de 51.15%, 33.1%,  9.5%, 6.1% (covarianzas). Así que como se dijo en el parrafo anterior las 2 primeras componentes puede resumir un 84.25% de varianza de las cuatro variables originales. 

Dicho todo esto, se podria realizar una reduccion a dos componentes usando covarianzas si es que con un minimo de 80% de la varianza estamos ok, o una reduccion a 3 componentes usando correlaciones si es que necesitamos explicar un minimo del 90%.

Esta conclusiones fueron realizada suponiendo que queremos por definicion explicar tanto porcentaje de la varianza, pero veamos ahora con dos de los criterios habituales para decidir el número de componentes principales a seleccionar: 

- la regla de Kaiser-Guttman y 

- el test de scree

## Eleccion del numero de componentes

### Componentes con Covarianzas

En el caso de realizar el PCA con la matriz de covarianzas, los valores propios son 83.95979 54.33519 15.73471 10.12013, y su valor medio es 41.03745. Aplicando el criterio de Kaiser-Gutman es suficiente con seleccionar 2 dos primeras componentes.

```{r}
ValoresPropios.cov = PCA.cov$sdev**2
ValoresPropios.cov
mean(ValoresPropios.cov)
```

Mientras que con el test de scree vemos que la curva comienza a suavizarse desde el 3, por lo que con las dos primeras componentes estariamos ok, incluso podria usarse la 3ra.
```{r}
screeplot(PCA.cov, type = c("lines"))
```


### Componentes con Correlaciones

Mientras que si aplicamos PCA con la matriz de correlaciones, sus valores propios son 1.8968406 1.3319478 0.5437291 0.2274825, y 1 es el valor medio. Según el criterio de Kaiser-Gutman se seleccionaría solo la primera componenete. Por lo que hay coincidencia con covarianzas.


```{r}
ValoresPropios.cor = PCA.cor$sdev**2
ValoresPropios.cor
mean(ValoresPropios.cor)
```
Y aqui con scree no queda muy claro donde esta el codo, pero podria ser en 3.

```{r}
screeplot(PCA.cor,  npcs = min(10, length(PCA.cor$sdev)),type = c("lines"))
```

### Representación de los datos en dimensión reducida

```{r}
# % de varianza explicada en cada componente principal, usando covarianzas
var.exp.cov <- (PCA.cov$sdev^2 / sum(PCA.cov$sdev^2))*100 

plot(PCA.cov$x[,1],PCA.cov$x[,2],
xlab = paste("CP1 (", round(var.exp.cov[1],2),"%)"),
ylab = paste("CP2 (", round(var.exp.cov[2],2),"%)"),
main = "Gráfico PCA basado en la covarianza",
col=c("lightblue")
 )

# % de varianza explicada en cada componente principal, usando correlaciones
var.exp.cor <- (PCA.cor$sdev^2 / sum(PCA.cor$sdev^2))*100 

plot(PCA.cor$x[,1],PCA.cor$x[,2],
xlab = paste("CP1 (", round(var.exp.cor[1],2),"%)"),
ylab = paste("CP2 (", round(var.exp.cor[2],2),"%)"),
main = "Gráfico PCA basado en la correlacion",
col=c("lightblue")
 )
```

Por lo que vemos aqui no es posible sacar ninguna conclusion con dos componentes principales sobre solo algunas (4 variables) columnas de nuestro dataset original. Habria que agregar mas variables al dataset para aplicar PCA e incluso alguna variables sobre la que querramos clasifcar en colores para poder asi discernir entre las observaciones sobre un valor de interes.

Veamos rapido una representacion agregando mas variables al aplicar PCA usando correlaciones y analicemos si obtenemos algun dato relevante.
```{r}
cols = c("Rating", "Height", "Weight", "Age", "Skill_Moves", "Ball_Control", "Dribbling", "Aggression", "Reactions", "Vision", "Stamina")
corrplot(cor(fifa2017[cols]), type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
pca_fifa2017 <- prcomp(x=fifa2017[cols], center = TRUE, scale.= TRUE)
summary(pca_fifa2017)

pca_fifa2017.cor <- (pca_fifa2017$sdev^2 / sum(pca_fifa2017$sdev^2))*100 
plot(pca_fifa2017$x[,1],pca_fifa2017$x[,2],
xlab = paste("CP1 (", round(pca_fifa2017.cor[1],2),"%)"),
ylab = paste("CP2 (", round(pca_fifa2017.cor[2],2),"%)"),
main = "Gráfico PCA basado en la correlacion",
col = c('lightblue')
 )

```

Entender el vector de loadings que forma cada componente nos puede ayudar a interpretar que clase de información capta cada componente, por ejemplo si miramos la componente 1:

```{r}
sort(pca_fifa2017$rotation[,'PC1'], decreasing =TRUE)
```
Vemos que para la primera componente son mas influyentes el Ball Control y el Dribbling y las menos influyentes son la altura y el peso.  Pero claro tengamos en cuenta que esta primer componente solo explica el 45% de la varianza, un valor muy bajo.

Esto lo podemos visualizar con un grafico de contribucion donde veremos mas simplemente las variables originales mas representadas en cada componente:
```{r}
#componente 1
fviz_contrib(pca_fifa2017,choice='var',axes=1, top = 10) 

#componente 2
fviz_contrib(pca_fifa2017,choice='var',axes=2, top = 10) 
```

Mismo podriamos combinar en un mismo grafico las dos primeras componentes para entender entre las dos cuales son las mas influyentes, veamoslo:

```{r}
fviz_cos2(pca_fifa2017,choice='var',axes=1:2)
```

Esto es importante saber ya que cuando contamos con muchas variables, podríamos decidir mostrar solo aquellas con mayor contribución.


Y asi podriamos seguir analizando el resto de las componentes para entender que variables tienen mas pesos sobre otras, o incluso agregando todas las variables del dataset original para que sea input de PCA e ir ajustando el analisis y entender si usando todas las caracteristicas de los jugadores nos permiten descubrir insights.

Veamos como se ven en un grafico de dos dimesiones al menos las primeras dos componentes, tanto aplicando pca con el dataset de 4 variables como el dataset de 11 variables que creamos posteriormente :

```{r}
biplot(x = PCA.cor, scale = 0, cex = 0.5, col = c("lightblue", "blue"))

biplot(x = pca_fifa2017, scale = 0, cex = 0.5, col = c("lightblue", "blue"))
```

En esta ultimo grafico vemos en la misma direccion 3 grupos de variables, varias que implican habilidad tecnica, otra la fuerza o agresividad deportiva, y la 3ra caracteristicas fisicas. Pero podemos ver que las caracteristicas fisicas nada tienen que ver con la habilidades de un jugador. Lo mismo que la edad, para nada tiene que ver con sus habilidades.

### Analisis de outliers
```{r}
#install.packages("remotes")
#remotes::install_github("vqv/ggbiplot")
library(ggbiplot)

biplot_outliers <- ggbiplot(pca_fifa2017, scale = 3, obs.scale = 3, labels = 1:17588)  +
ggtitle("Análisis de Componentes Principales")
biplot_outliers
```

Los puntos mas alejados de la nube son el 1, 2, 4, 9, 32 y 8359. Veamos estas observaciones en el dataset:

```{r}
rows = c(1, 2, 4, 9, 32, 8359)
fifa2017[rows,]
```

Mas alla que 3 de estos outliers sean observaciones a las que les hemos estimado peso o altura, que sean outliers la causa pueda ser que en escencia son jugadores atipicos y mucho mejores que la "media". Mientras que el 8359, si bien su peso y altura son posibles en la realidad, no son tan tipicas en el mundo de los jugadores de futbol, pero claro es un Arquero, en general son altos, pero este lo es bastante.
A pesar de tener nosotros este conocimiento externo de las caracteristica de los jugadores, en casos de muestras no tan conocidas, una opcion seria volver a realizar PCA con todas las variables numericas como comentamos antes o realizar PCA quitando los "outliers" que tambien comentamos antes, o incluso combinacion de ambas. 

Tal como estamos descubriendo el mundo del analisis de los datos no es un proceso estatico o no existe una "receta" estrica a seguir para obtener un resultado preciso. Es un proceso continuo e iterativo el cual hay que seguir hasta obtener posibles insights o el objetivo deseado.

Al usar varianzas PCA es sensible a los outliers, es por eso que existen alternativas de PCA basadas en estimadores robustos tal como es ejemplificado en esta publicacion: **https://rpubs.com/Joaquin_AR/578072**

# 10. Archivo final

El resultado del pre procesamiento del archivo fifa.csv se guarda en el archivo “apellido_fichero_clean.csv”.

```{r}
write.csv(fifa2017, 'delgado_fichero_clean.csv', row.names = FALSE)
```


# Referencias

https://www.rdocumentation.org/packages/stringr/versions/1.4.0

https://stackoverflow.com/questions/6364783/capitalize-the-first-letter-of-both-words-in-a-two-word-string

https://rdrr.io/cran/eeptools/man/age_calc.html

https://www.rdocumentation.org/packages/psych/versions/2.0.12

