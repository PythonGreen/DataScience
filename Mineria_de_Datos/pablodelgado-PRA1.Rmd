---
title: 'Minería de datos: PRA1 - Selección y preparación de un juego de datos'
author: "Autor: Nombre estudiante"
date: "Noviembre 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: pablodelgado-PRA1-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
******
# Introducción
******
## Presentación
Esta práctica cubre de forma transversal la asignatura.

Las Prácticas 1 y 2 de la asignatura se plantean de una forma conjunta de modo que la Práctica 2 será continuación de la 1.

El objetivo global de las dos prácticas consiste en seleccionar uno o varios juegos de datos, realizar las tareas de **preparación y análisis exploratorio** con el objetivo de disponer de datos listos para **aplicar algoritmos** de clustering, asociación y clasificación.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación de todos los aspectos trabajados durante el semestre.  
En esta práctica abordamos un caso real de minería de datos donde tenemos que poner en juego todos los conceptos trabajados.
Hay que trabajar todo el ciclo de vida del proyecto. Desde el objetivo del proyecto hasta la implementación del conocimiento encontrado pasando por la preparación, limpieza de los datos, conocimiento de los datos, generación del modelo, interpretación y evaluación.

## Descripción de la PRA a realizar

## Recursos Básicos
Material docente proporcionado por la UOC. 

## Criterios de valoración

**Ejercicios prácticos** 

Para todas las PRA es **necesario documentar** en cada apartado del ejercicio práctico que se ha hecho y como se ha hecho.

## Formato y fecha de entrega PRA_1
El formato de entrega es: usernameestudiant-PRAn.html/doc/docx/odt/pdf  
Fecha de entrega: 02/12/2020  
Se debe entregar la PRA_1 en el buzón de entregas del aula  

## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra esta protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra esta protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado
******
Todo estudio analítico debe nacer de una necesidad por parte del **negocio** o de una voluntad de dotarle de un conocimiento contenido en los datos y que solo podremos obtener a través de una colección de buenas prácticas basadas en la Minería de Datos.  

El mundo de la analítica de datos se sustenta en 3 ejes:  

1. Uno de ellos es el profundo **conocimiento** que deberíamos tener **del negocio** al que tratamos de dar respuestas mediante los estudios analíticos.  

2. El otro gran eje es sin duda las **capacidades analíticas** que seamos capaces de desplegar y en este sentido, las dos prácticas de esta asignatura pretenden que el estudiante realice un recorrido sólido por este segundo eje.  

3. El tercer eje son los **Datos**. Las necesidades del Negocio deben concretarse con preguntas analíticas que a su vez sean viables responder a partir de los datos de que disponemos. La tarea de analizar los datos es sin duda importante, pero la tarea de identificarlos y obtenerlos va a ser para un analista un reto permanente.  

Como **primera parte** del estudio analítico que nos disponemos a realizar, se pide al estudiante que complete los siguientes pasos:   

1. Seleccionar un juego de datos y justificar su elección. El juego de datos deberá tener capacidades para que se le puedan aplicar algoritmos supervisados, algoritmos no supervisados y reglas de asociación.   

2. Realizar un análisis exploratorio del juego de datos seleccionado.   

3. Realizar tareas de limpieza y acondicionado para poder ser usado en procesos de modelado.

4. Realizar métodos de discretización

5. Aplicar un estudio PCA sobre el juego de datos. A pesar de no estar explicado en el material didáctico, se valorará si en lugar de PCA investigáis por vuestra cuenta y aplicáis SVD (Single Value Decomposition).

******
## Seleccion del dataset
******

> El dataset elegido para esta practica sera el de ausentismo en el trabajo descargado del **UC Irvine Machine Learning Repository**:

https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work


Abstract: The database was created with records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil.
![DataSetDescription](Absenteeism.JPG)

Tal como se comenta en su descripcion este dataset permite realizar tareas de clasificacion y clustering. En cuanto a las reglas de asociacion se podran realizar algunas tareas de "ajuste" sobre el dataset para permitir su aplicacion, por ej: convirtiendo algunas variables categoricas en transacciones para intentar buscar relaciones y/o patrones en los datos que sean utiles.

> Sumamos aqui informacion del dataset y de los atributos existente en el sitio UCI:



* Data Set Information:
The data set allows for several new combinations of attributes and attribute exclusions, or the modification of the attribute type (categorical, integer, or real) depending on the purpose of the research.The data set (Absenteeism at work - Part I) was used in academic research at the Universidade Nove de Julho - Postgraduate Program in Informatics and Knowledge Management.

* Attribute Information:

		1. Individual identification (ID)
		2. Reason for absence (ICD). 
		3. Month of absence
		4. Day of the week 
		5. Seasons 
		6. Transportation expense
		7. Distance from Residence to Work (kilometers)
		8. Service time
		9. Age
		10. Work load Average/day
		11. Hit target
		12. Disciplinary failure
		13. Education 
		14. Son (number of children)
		15. Social drinker 
		16. Social smoker 
		17. Pet (number of pet)
		18. Weight
		19. Height
		20. Body mass index
		21. Absenteeism time in hours (target)      


> Todas los valores descriptivos o categoricos en las observaciones ya vienen convertidos a numericos en el dataset original. Aqui un detalle de los mismos:

**Reason for absence**

        1. Certain infectious and parasitic diseases
        2. II Neoplasms
        3. Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism
        4. Endocrine, nutritional and metabolic diseases
        5. Mental and behavioural disorders
        6. Diseases of the nervous system
        7. Diseases of the eye and adnexa
        8. Diseases of the ear and mastoid process
        9. Diseases of the circulatory system
        10. Diseases of the respiratory system
        11. Diseases of the digestive system
        12. Diseases of the skin and subcutaneous tissue
        13. Diseases of the musculoskeletal system and connective tissue
        14. Diseases of the genitourinary system
        15. Pregnancy, childbirth and the puerperium
        16. Certain conditions originating in the perinatal period
        17. Congenital malformations, deformations and chromosomal abnormalities
        18. Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified
        19. Injury, poisoning and certain other consequences of external causes
        20. External causes of morbidity and mortality
        21. Factors influencing health status and contact with health services.
        And 7 categories without (CID): 
          patient follow-up (22), 
          medical consultation (23), 
          blood donation (24), 
          laboratory examination (25), 
          unjustified absence (26), 
          physiotherapy (27), 
          dental consultation (28).		  
**Day of the week**

    (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))
**Seasons**

    (summer (1), autumn (2), winter (3), spring (4))
**education**

    (high school (1), graduate (2), postgraduate (3), master and doctor (4))
**Disciplinary failure**

    (yes=1; no=0)
**Social drinker**

    (yes=1; no=0)
**Social smoker**

    (yes=1; no=0)

Lo interesante de este dataset es la posibilidad de preveer a futuro las posibles ausencias de los empleados de una empresa. Hoy dia, si consideramos por ejemplo las metodologias agiles, donde de antemano se prevee la capacidad con la que contara el equipo en el proximo sprint, que mejor tener como ayuda para un scrum master una prevision de ausencias segun reglas prestablecidas de acuerdo al comportamiento general de un miembro del equipo? 

Podriamos entender por ejemplos: porque se dan las ausencias? por temas personales? segun en que estacion del año estamos? podemos segmentar estos analisis segun las caracteristicas o habitos de ciertos grupos de empleados?

Pensemoslo no solo para establecer la capacidad del equipo, imaginemos si conocer estos posibles patrones de comportamiento podrian ayudar a RRHH a mejorar la seleccion de proximos candidatos a un puesto? o mismo la contratacion de personal temporal segun la demanda estacional de proyectos dentro de una empresa y contrastandolo con la cantidad posibles de horas de aunsencia del personal actual.

Por lo que, con los algoritmos de clustering intentaremos encontrar grupos similares de usuarios y determinar cual es el numero optimo para segmentarlos y poder luego hacer tratamientos diferenciados.

Con los algoritmos de asociacion buscaremos a partir de los datos reglas o patrones que nos permitan tener mas informacion acerca del comportamiento de los empleados que nos den mas informacion de posibles cuestiones a tener en cuenta o patrones, como dijimos, en el comportamiento a la hora de ausentarse o la relacion que hay con la cantidad de horas en las que se ausentan.

Mientras que por ultimo con los algoritmos de clasificacion, con todo lo analizado previamente buscaremos predecir el comportamiento futuro del empleado o las horas que podran llegar a ausentarse en el futuro.

******
## EDA I & Limpieza/Preparacion de Datos
******

Dado que el archivo es un csv y solo tiene 740 observaciones, primero hacemos una rapida inspeccion manual. Como hemos dicho anteriormente posee todos valores numericos, tenemos encabezado de columnas y todos los valores de cada fila estan separados por punto y coma. Todos los numeros son enteros a excepcion al parecer del work load average. Mientras que por ej la estatura y el peso estan expresados en cm y kilos respectivamente.

Dicho eso comenzaremos con el analisis exploratorio del dataset. Primero que nada importamos todas las librerias que estaremos usando o preveemos utilizar tanto en esta practica como en la siguiente.

```{r}

#install.packages("factoextra")

library(readr)      # importing files library
library(dplyr)      # manipulating data library
library(ggplot2)    # visualizing data library
library(factoextra) # for plotting and validatin/g clusters
library(gridExtra)  # Arranging multiple grid-based plots on a page
library(cluster)    # Clustering  library
library(arules)     # Associations rules library
library(reshape2)   # Reshaping data for ggplot
```

Creamos el dataset a partir del archivo:
```{r}
# Leemos el archivo csv delimitado por ;
abs_df <- read_delim("Absenteeism_at_work.csv", col_names = TRUE, delim=';')
```
Hacemos las primeros chequeos del contenido del dataset:
```{r}
# visualizamos las primeras 5 observaciones
head(abs_df,5)

# Estadisticas basicas 
summary(abs_df)

# Verificamos la estructura y contenido del conjunto de datos
str(abs_df)
```

```{r}
# Dado que los nombres de las columnas tienes espacios, los quitamos agregando puntos:
names(abs_df)<-make.names(names(abs_df),unique = TRUE)

# Chequeamos ahora los nuevos nombres de columnas
colnames(abs_df)
```

Si bien, segun la descripcion del dataset en UCI y con la inspeccion visual no hay missing values, realicemos un chequeo rapido:

```{r}
# Estadísticas de valores vacíos
colSums(is.na(abs_df))
# y ahora los missing
colSums(abs_df=="")
```

Ahora analicemos algunas variables numerica y visualmente para entender mejor los datos:

```{r}
# Veamos los valores distintos de cada atributo
sapply(abs_df, function(x) length(unique(x)))
```
Aqui vemos que tenemos 13 meses informados de ausencia, arranquemos analizando esa variable con graficas simples.

Probemos tambien dos mas, la educacion de los empleados con la cantidad de horas de ausentimos


```{r}

a = ggplot(abs_df,aes(x=Month.of.absence,fill=Month.of.absence))+geom_bar(fill="lightblue")+theme_bw()

b = ggplot(abs_df,aes(x=Education,fill=Education))+geom_bar(fill="lightblue")+theme_bw()

c = ggplot(abs_df,aes(x=Absenteeism.time.in.hours,fill=Absenteeism.time.in.hours))+geom_bar(fill="lightblue")+theme_bw()

grid.arrange(a, b, nrow = 1, ncol = 2)

grid.arrange(c, nrow = 1)

# vemos que los datos para ese mes no informado, son pocos, solo 3 obs.
table(abs_df$Month.of.absence)

# Chequeemos esos datos:
select(filter(abs_df, Month.of.absence ==0), ID,Reason.for.absence,Day.of.the.week, Education, Absenteeism.time.in.hours)
```
En cuanto a los datos con mes no informado, luego podriamos eliminar estos registros ya que como vemos:

1 tienen razon de ausentismo en 0 y ademas la cantidad de horas informadas es cero, eso indica que son personas que nunca han faltado y por eso no hay ninguna razon de ausentismo???

2 son datos del nivel de educacion 1 que corresponde al que mayor cantidad de registros tenemos, por lo que quitar 3 filas de el no deberia representar un sesgo.

Mientras que vemos en la grafica que para la variable education predominan las de valor 1. Pero claro si bien tener las variables numericas nos permite utilizar varias funciones y aplicar algoritmos, para un analisis exploratorio inicial seria bueno tener los valores descriptivos reales para entender "mas facil" los datos.

Asi que iremos agregando variables al dataset con las descripciones de estas variables numericas. 

  (high school (1), graduate (2), postgraduate (3), master and doctor (4))
  
Antes de comenzar a agregar columnas descriptivas, chequemos el punto 1 que acabamos de comentar.
  
```{r}
# Validamos para los reason 0, si hay horas de ausencia
table(filter(abs_df, Reason.for.absence ==0)$Absenteeism.time.in.hours)

# Validamos para los que no tienen horas informadas que tengan 0 en reason
table(filter(abs_df, Absenteeism.time.in.hours == 0)$Reason.for.absence)
```

Bueno, al parecer el reason 0 se corresponde a los que no se han ausentando al trabajo, salvo solo 1 obversacion donde tiene un reason de ausencia pero no ha informado horas. Por lo no afectara a nuestros analisis estos registros.
Con esto tambien revalidamos que necesitamos tener disponibles los valores descriptivos para este tipo de analisis exploratorio que estamos haciendo ya que sino  tendriamos que ir a buscar cada vez a que descripcion corresponde cada ID de variable.

```{r}
# Creamos Seasons Desc
abs_df <- abs_df %>% mutate(Reason.for.absence.Desc = case_when
                   (Reason.for.absence == 0 ~ 'No Aplica',
                    Reason.for.absence == 1 ~ 'Infectious',
                    Reason.for.absence == 2 ~ 'Neoplasms',
                    Reason.for.absence == 3 ~ 'Immune System & Blood Issues',
                    Reason.for.absence == 4 ~ 'Metabolic diseases',
                    Reason.for.absence == 5 ~ 'Mental & Behavior disorders',
                    Reason.for.absence == 6 ~ 'nervous system diseases',
                    Reason.for.absence == 7 ~ 'eye and adnexa diseases',
                    Reason.for.absence == 8 ~ 'ear and mastoid diseases',
                    Reason.for.absence == 9 ~ 'circulatory diseases',
                    Reason.for.absence == 10 ~ 'respiratory diseases',
                    Reason.for.absence == 11 ~ 'digestive diseases',
                    Reason.for.absence == 12 ~ 'skin diseases',
                    Reason.for.absence == 13 ~ 'musculoskeletal diseases',
                    Reason.for.absence == 14 ~ 'genitourinary diseases',
                    Reason.for.absence == 15 ~ 'Pregnancy, and related',
                    Reason.for.absence == 16 ~ 'perinatal conditions',
                    Reason.for.absence == 17 ~ 'Congenital malformations',
                    Reason.for.absence == 18 ~ 'abnormal clinical findings',
                    Reason.for.absence == 19 ~ 'Injury, poisoning and related',
                    Reason.for.absence == 20 ~ 'morbidity and mortality',
                    Reason.for.absence == 21 ~ 'Other Factors',
                    Reason.for.absence == 22 ~ 'patient follow-up',
                    Reason.for.absence == 23 ~ 'medical consultation',
                    Reason.for.absence == 24 ~ 'blood donation',
                    Reason.for.absence == 25 ~ 'laboratory examination',
                    Reason.for.absence == 26 ~ 'unjustified absence',
                    Reason.for.absence == 27 ~ 'physiotherapy',
                    Reason.for.absence == 28 ~ 'dental consultation'
                    )
                  )


# Creamos Seasons Desc
abs_df <- abs_df %>% mutate(Seasons.Desc = case_when
                  (Seasons == 1 ~ 'Summer',
                   Seasons == 2 ~ 'Autumn',
                   Seasons == 3 ~ 'Winter',
                   Seasons == 4 ~ 'Spring')
                  )

# Creamos Education Desc
abs_df <- abs_df %>% mutate(Education.Desc = case_when
                  (Education == 1 ~ 'HighSchool',
                   Education == 2 ~ 'Graduate',
                   Education == 3 ~ 'PostGraduate',
                   Education == 4 ~ 'Ms & Dr')
                  )

# Mientras que variables como Disciplinary failure, Social drinker y Social smoker no haran falta convertirlas a descriptivas ya que es claro que 1 es Yes y 0 es No. Lo mismo sucede con Day of the week, es simple determinar de que estamos hablando cuando vemos los valores numericos. Caso similar con month, todos sabemos que mes representan los valores del 1 al 12, siendo el 0 lo que comentamos antes, valores no informados que no afectaran al modelo, aunque haremos nuestros testeos mas adelante en la practica.

```


  
Dicho eso y luego de haber agregado los campos descriptivos sigamos analizando el resto de las variables:


```{r}

pet = ggplot(abs_df,aes(x=Pet,fill=Pet))+geom_bar(fill="lightblue")+theme_bw()
Seasons = ggplot(abs_df,aes(x=Seasons.Desc,fill=Seasons.Desc))+geom_bar(fill="lightblue")+theme_bw()

age = ggplot(abs_df,aes(x=Age,fill=Age))+geom_bar(fill="lightblue")+theme_bw()
day = ggplot(abs_df,aes(x=Day.of.the.week,fill=Day.of.the.week))+geom_bar(fill="lightblue")+theme_bw()

disciplinary = ggplot(abs_df,aes(x=Disciplinary.failure,fill=Disciplinary.failure))+geom_bar(fill="lightblue")+theme_bw()
distance = ggplot(abs_df,aes(x=Distance.from.Residence.to.Work,fill=Distance.from.Residence.to.Work))+geom_bar(fill="lightblue")+theme_bw()

drinker = ggplot(abs_df,aes(x=Social.drinker,fill=Social.drinker))+geom_bar(fill="lightblue")+theme_bw()
smoker = ggplot(abs_df,aes(x=Social.smoker,fill=Social.smoker))+geom_bar(fill="lightblue")+theme_bw()


weight = ggplot(abs_df,aes(x=Weight,fill=Weight))+geom_bar(fill="lightblue")+theme_bw()
mass = ggplot(abs_df,aes(x=Body.mass.index,fill=Body.mass.index))+geom_bar(fill="lightblue")+theme_bw()

grid.arrange(pet,Seasons, age, day, disciplinary, distance, drinker, smoker, weight, mass, nrow = 5, ncol = 2)

#library(forcats)
reason = ggplot(abs_df) +
    geom_bar(aes(x = forcats::fct_infreq(Reason.for.absence.Desc)
                 #,fill="Education"
                 ), fill="lightblue")+theme_bw()+theme(axis.text.x=element_text(size=8,angle=90))

grid.arrange(reason, nrow = 1)


absenteeism = ggplot(abs_df)+geom_bar(aes(x=forcats::fct_infreq(as.factor(Absenteeism.time.in.hours) )),fill="lightblue")+theme_bw()

grid.arrange(absenteeism, nrow = 1)

```

******
## Discretizacion
******

Ademas de comenzar a sacar conclusiones sobres los datos y empezar a cruzar variables podriamos discretizar variables como:

* Age
* Distance.from.Residence.to.Work
* Body.mass.index
* Absenteeism.time.in.hours

Ya que para age ya hemos visto que tenemos bastante dispersos las edades y mejor agruparlas por los clasicos rangos de edad. Con la distancia nos paso algo similar.

Para Body mass index al ser un indice nos dice poco sin contrastarlo con algo que conozcamos asi que mejor agruparlo segun los criterios que se los suele analizar a nivel medico:

    < 18.5 Underweight
    18.5-25 Normal weight
    25-30 Overweight
    > 30  Obese

Mientras que las horas de ausentismo vimos en la grafica que la mayor cantidad de empleados faltan pocas horas y mejor armar grupos para el analisis:
    
    0 h horas
    1-3 horas
    4-8 horas
    9-16 horas
    17-40 horas 
    + 40 horas
                    
```{r}
# Creamos age range
abs_df["age_range"] <- cut(abs_df$Age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))

# Creamos Distance range
abs_df["distance_range"] <- cut(abs_df$Distance.from.Residence.to.Work, breaks = c(0,10,20,30,40,100), labels = c("10km", "20km", "30km", "40km", "+40km"))

# Creamos BMI
abs_df["BMI"] <- cut(abs_df$Body.mass.index, breaks = c(0,18.5,25,30,100), labels = c("Underweight", "Normal", "Overweight", "Obese"))

# Creamos absenteeism_range
abs_df <- abs_df %>% mutate(absenteeism_range = case_when
                   (Absenteeism.time.in.hours == 0 ~ '0 h',
                    between(Absenteeism.time.in.hours, 1, 3) ~ '1-3 h',
                    between(Absenteeism.time.in.hours, 4, 8) ~ '4-8 h',
                    between(Absenteeism.time.in.hours, 9, 16) ~  '9-16 h',
                    between(Absenteeism.time.in.hours, 17, 40) ~ '17-40 h',
                    Absenteeism.time.in.hours > 40 ~ '+ 40 h'
                   )
                )
```

Vemos ahora como quedaron distribuidos los datos de estas variables post discretizacion


```{r}

# Comparacion age y age discretizada
ant = ggplot(abs_df,aes(x=Age,fill=Age))+geom_bar(fill="lightblue")+xlab("Age Before")+theme_bw()
desp = ggplot(abs_df,aes(x=age_range,fill=age_range))+geom_bar(fill="lightblue")+xlab("Age After")+theme_bw()
grid.arrange(ant,desp, nrow = 1, ncol = 2)

# Distancia y Distancia discretizada
ant = ggplot(abs_df,aes(x=Distance.from.Residence.to.Work,fill=Distance.from.Residence.to.Work))+geom_bar(fill="lightblue")+xlab("Distance Before")+theme_bw()
desp = ggplot(abs_df,aes(x=distance_range,fill=distance_range))+geom_bar(fill="lightblue")+xlab("Distance After")+theme_bw()
grid.arrange(ant,desp, nrow = 1, ncol = 2)

# BMI y BMI discretizada
ant = ggplot(abs_df,aes(x=Body.mass.index,fill=Body.mass.index))+geom_bar(fill="lightblue")+xlab("BMI Before")+theme_bw()
desp = ggplot(abs_df,aes(x=BMI,fill=BMI))+geom_bar(fill="lightblue")+xlab("BMI After")+theme_bw()
grid.arrange(ant,desp, nrow = 1, ncol = 2)


# Horas de Ausentimos discretizada
ant = ggplot(abs_df)+geom_bar(aes(x=forcats::fct_infreq(as.factor(Absenteeism.time.in.hours) )),fill="lightblue")+xlab("Absenteeism Before")+theme_bw()

desp = ggplot(abs_df)+geom_bar(aes(x=forcats::fct_infreq(as.factor(absenteeism_range) )),fill="lightblue")+xlab("Absenteeism After")+theme_bw() 
 
grid.arrange(ant,desp, nrow = 1, ncol = 2)
```

******
## EDA II
******

Comencemos ahora a contrastar variables, encontrar correlaciones, etc.. en definitiva entender los datos.



******
## PCA
******
Si bien a grandes razgos los pasos para aplicar PCA son:

1. Estandarizar variables

2. Calcular la matriz de covarianzas MC

3. Generar los eigenvectores y eigenvalores a partir de la matriz MC

4. Ordenar los eigenvectores a partir de los eigenvalores de manera descendente y quedarnos con los TOP **k** eigenvectores

5. Contruir la matriz de projecciones MP, a partir de los eigenvectores seleccionados

6. Transformar el dataset original a partir de la matriz MP para obtener las nuevas dimensiones.

Nos apoyaremos en las funciones existentes en R que permiten calcular directamente las componentes principales y los principal component scores de cada observación sin tener que ir paso por paso.

Comencemos calculando la varianza de las variables del dataset:

```{r}
sort(apply(X = abs_df[,2:20], MARGIN = 2, FUN = var), decreasing =TRUE)
```
Como vemos tenemos valores muy extremos de varianzas en el orden de los 4500 para los gastos de viaje hasta llegar a valores en el orden de los 0.05 para los fallos disciplinarios. Deberemos estandarizar las variables para que tengan media cero y desviación estándar 1 antes de realizar el estudio PCA.



```{r}
# Con esta funcion prcomp podemos estandarizar las variables y hacer que la desviacion estandar sea 1 con el parametro scale=TRUE
pca <- prcomp(abs_df[,2:20], scale = TRUE)
names(pca)
```


```{r}
# Donde la variable center contiene la media de cada variable antes de estandarizar
sort(pca$center, decreasing =TRUE)

# Tenemos lo mismo, pero en este caso la desviacion estandar pre estandarizacion en scale
sort(pca$scale, decreasing =TRUE)

# Mientras que rotation contiene el valor de los loadings ϕ para cada componente (eigenvector). El número de componentes principales se corresponde con el mínimo(n-1,p), que en este caso es  min(740,19)=19
pca_rot = pca$rotation

# veamos algunas registros:
head(pca_rot,5)


# Ahora si las varianzas estan mas igualadas post estandarizacion:
apply(X = pca_rot, MARGIN = 2, FUN = var)
```
Entender el vector de loadings que forma cada componente nos puede ayudar a interpretar que clase de información capta cada componente, por ejemplo si miramos la componente 1:

```{r}
# lo que vemos es que la primer componente capta mas informacion de la variable Education, cantidad de mascotas y gastos de viaje positivamente y negativamente del BMI, Weight y Service Time.
sort(pca_rot[,'PC1'], decreasing =TRUE)

# y asi podriamos seguir analizando el resto de las componentes para entender que variables tienen mas pesos sobre otras.
```


Otra de las variables que genera la funcion prcomp es la matriz x, la cual es el resultado de multiplicar los datos por los loadings
```{r}
head(pca$x)

# 740 valores x 19 componentes
dim(pca$x)
```

Veamos como se ven en un grafico de dos dimesiones al menos las primeras dos componentes:

```{r}
biplot(x = pca, scale = 0, cex = 0.5, col = c("lightblue", "blue"))
```

Ya obtenidas las componentes principales, se puede saber cual es la varianza explicada por cada una de ellas, la proporción respecto a la varianza total y la proporción acumulada de la varianza.

```{r}
proporcion_varianza <- pca$sdev^2 / sum(pca$sdev^2)
proporcion_varianza

```
```{r}
ggplot(data = data.frame(proporcion_varianza, pc = 1:19),
       aes(x = pc, y = proporcion_varianza)) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.2)) +
  theme_bw() +
  labs(x = "Componentes principales",
       y = "Proporcion de la varianza explicada")

```
```{r}
proporcion_varianza_acum <- cumsum(proporcion_varianza)
proporcion_varianza_acum
```
```{r}
ggplot(data = data.frame(proporcion_varianza_acum, pc = 1:19),
       aes(x = pc, y = proporcion_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Componentes principales",
       y = "Proporcion acumulada de la varianza explicada ")

```

**Dicho todo esto, si quisieramos por ejemplo explicar al menos un minimo de 70% de la varianza deberiamos usar las primeras 8 componentes principales.** o 10 si quisieramos explicar el 80%.


******
# Rúbrica
******
* 25%. Justificación de la elección del juego de datos donde se detalle el potencial analítico que se intuye. El estudiante deberá visitar los siguientes portales de datos abiertos para seleccionar su juego de datos:
  + [Datos.gob.es](https://datos.gob.es/es/catalogo?q=&frequency=%7B"type"%3A+"months"%2C+"value"%3A+"1"%7D&sort=score+desc%2C+metadata_modified+desc)
  + [UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets.php)
  + [Datasets Wikipedia](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)
  + [Datos abiertos Madrid](https://datos.madrid.es/portal/site/egob/)
  + [Datos abiertos Barcelona](https://opendata-ajuntament.barcelona.cat/es/)
  + [London Datastore](https://data.london.gov.uk/)
  + [NYC OpenData](https://opendata.cityofnewyork.us/)
* 25%. Información extraída del análisis exploratorio. Distribuciones, correlaciones, anomalías,... 
* 25%. Explicación clara de cualquier tarea de limpieza o acondicionado que se realiza. Justificando el motivo y mencionando las ventajas de la acción tomada.
* 25%. Se realiza un proceso de PCA o SVD donde se aprecia mediante explicaciones y comentarios que el estudiante entiende todos los pasos y se Scomenta extensamente el resultado final obtenido.


******
# Recursos de programación
******
* Incluimos en este apartado una lista de recursos de programación para minería de datos donde podréis encontrar ejemplos, ideas e inspiración:
  + [Material adicional del libro: Minería de datos Modelos y Algoritmos](http://oer.uoc.edu/libroMD/)
  + [Espacio de recursos UOC para ciencia de datos](http://datascience.recursos.uoc.edu/es/)
  + [Buscador de código R](https://rseek.org/)  
  + [Colección de cheatsheets en R](https://rstudio.com/resources/cheatsheets/)  
  

******

******


