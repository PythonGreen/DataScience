---
title: 'Minería de datos: PRA2 - Modelado de un juego de datos'
author: "Autor: Pablo A. Delgado"
date: "Diciembre 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 4
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
******
# Introducción
******
## Presentación
Esta práctica cubre de forma transversal la asignatura.

Las Prácticas 1 y 2 de la asignatura se plantean de una forma conjunta de modo que la Práctica 2 será continuación de la 1.

El objetivo global de las dos prácticas consiste en seleccionar uno o varios juegos de datos, realizar las tareas de preparación y análisis exploratorio con el objetivo de disponer de datos listos para aplicar algoritmos de clustering, asociación y clasificación.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación de todos los aspectos trabajados durante el semestre.  
En esta práctica abordamos un caso real de minería de datos donde tenemos que poner en juego todos los conceptos trabajados.
Hay que trabajar todo el ciclo de vida del proyecto. Desde el objetivo del proyecto hasta la implementación del conocimiento encontrado pasando por la preparación, limpieza de los datos, conocimiento de los datos, generación del modelo, interpretación y evaluación.

## Descripción de la PEC a realizar

## Recursos Básicos
Material docente proporcionado por la UOC. 

## Criterios de valoración

**Ejercicios prácticos** 

Para todas las PEC es necesario documentar en cada apartado del ejercicio práctico que se ha hecho y como se ha hecho.

## Formato y fecha de entrega
El formato de entrega es: usernameestudiante-PECn.html/doc/docx/odt/pdf/rmd  
Fecha de entrega: 15/01/2020  
Se debe entregar la PEC en el buzón de entregas del aula  

## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra esta protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra esta protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado
******
Como continuación del estudio iniciado en la práctica 1, procedemos en esta práctica 2 a aplicar modelos analíticos sobe el juego de datos seleccionado y preparado en la práctica anterior.

De este modo se pide al estudiante que complete los siguientes pasos: 

1. Tratar la práctica como un proyecto real de minería de datos, con todos los puntos de su **ciclo de vida**.

2. Aplicar un modelo de generación de reglas a partir de **árboles de decisión**.  

3. Aplicar un modelo **no supervisado** y basado en el concepto de **distancia**, sobre el juego de datos.   

4. Aplica de nuevo el modelo anterior, pero usando una **métrica distinta** y compara los resultados.

5. Aplicar un **modelo supervisado** sobre el juego de datos **sin** haber aplicado previamente **PCA/SVD**.

6. Aplicar un **modelo supervisado** sobre el juego de datos habiendo aplicado previamente **PCA/SVD**.

7. ¿Ha habido mejora en capacidad predictiva, tras aplicar PCA/SVD? ¿A qué crees que es debido?.   


******
# Resolucion
******

Tal como se solicita en el enunciado trataremos la resolucion de esta practica en conjunto con lo desarrollado en la Practica1 y ademas siguiendo las etapas del ciclo de vida de un proyecto real de datamining. Es por eso que ante todo los describimos brevemente aqui:

Datamining debe considerarse como un proceso continuo que integra los siguientes aspectos: 

a) **Definición del objetivo** del proyecto de data mining, precisando la tarea principal que hay que realizar y eligiendo el método más adecuado según las circunstancias.

b) **Selección de los datos** relevantes.

c) **Preparación de los datos** de cara a asegurar que sean válidos y se encuentren en condiciones de ser utilizados por el método seleccionado.

d) **Data mining propiamente dicho**, es decir, aplicación sobre los datos ya preparados del método elegido y construcción del modelo correspondiente.

e) **Evaluacion e interpretación del modelo** obtenido, que puede provocar la revisión de algunas de las fases anteriores.

f) **Integración en el sistema** de tratamiento de información, que comprende la observación del rendimiento y, en caso de cambio del entorno o “envejecimiento” del modelo, inicio de un proceso de data mining nuevo. Es en esta etapa donde finalmente se responde al objetivo planteado, donde se resuelve la situacion de negocio o el problema definido, donde como dijimos siempre se puede volver a replantear el objetivo o haberse modificado o alterado el issue inicial por el cual se puede iniciar un nuevo proceso de datamining.

Dicho esto iniciemos el proceso

## Definicion del objetivo

Dado que actualmente la demanda de las empresas para atender los diferentes objetivos que se le plantean son cada vez mas criticos y contar con todo el personal es clave para resolverlos o llevarlos a cabo, ideal seria poder preveer cualquier situacion futura que impida contar con todos los recursos y poder gestionar los proyectos y evitar riesgos de entrega, implementacion o puesta en produccion de los mismos.

Es por eso que se desea a traves del dataset disponible descripto mas abajo poder determinar o predecir el ausentismo de los recursos humano de una compania.

Incluso, bajando mas de nivel dando una ejemplo mas concreto, si consideramos las metodologias agiles, donde de antemano se prevee la capacidad con la que contara el equipo en el proximo sprint, que mejor que tener como ayuda para un scrum master una prevision de ausencias segun reglas prestablecidas de acuerdo al comportamiento general de un miembro del equipo?

Podriamos entender por ejemplos: porque se dan las ausencias? por temas personales? segun en que estacion del año estamos? podemos segmentar estos analisis segun las caracteristicas o habitos de ciertos grupos de empleados?

Pensemoslo no solo para establecer la capacidad del equipo, imaginemos si conocer estos posibles patrones de comportamiento podrian ayudar a RRHH a mejorar la seleccion de proximos candidatos a un puesto? o mismo la contratacion de personal temporal segun la demanda estacional de proyectos dentro de una empresa y contrastandolo con la cantidad posibles de horas de aunsencia del personal actual.

Por lo que, con los algoritmos de clustering, trataremos de encontrar grupos similares de usuarios y determinar cual es el numero optimo para segmentarlos y poder luego hacer tratamientos diferenciados.

Luego intentaremos buscar a partir de los datos reglas o patrones que nos permitan tener mas informacion acerca del comportamiento de los empleados que nos den mas informacion de posibles cuestiones a tener en cuenta o patrones, como dijimos, en el comportamiento a la hora de ausentarse o la relacion que hay con la cantidad de horas en las que se ausentan.

Para ello con los algoritmos de clasificacion, con todo lo analizado previamente buscaremos predecir el comportamiento futuro del empleado o las horas que podran llegar a ausentarse en el futuro.



## Reunir los datos 

Los datos que utilizaremos para esta tarea sera el obtenido de descargado del UC Irvine Machine Learning Repository:

https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work

Abstract: The database was created with records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil.
![DataSetDescription](Absenteeism.JPG)

Tal como se comenta en su descripcion este dataset permite realizar tareas de clasificacion y clustering. 

* Data Set Information:
The data set allows for several new combinations of attributes and attribute exclusions, or the modification of the attribute type (categorical, integer, or real) depending on the purpose of the research.The data set (Absenteeism at work - Part I) was used in academic research at the Universidade Nove de Julho - Postgraduate Program in Informatics and Knowledge Management.

* Attribute Information:

		1. Individual identification (ID)
		2. Reason for absence (ICD). 
		3. Month of absence
		4. Day of the week 
		5. Seasons 
		6. Transportation expense
		7. Distance from Residence to Work (kilometers)
		8. Service time
		9. Age
		10. Work load Average/day
		11. Hit target
		12. Disciplinary failure
		13. Education 
		14. Son (number of children)
		15. Social drinker 
		16. Social smoker 
		17. Pet (number of pet)
		18. Weight
		19. Height
		20. Body mass index
		21. Absenteeism time in hours (target)      


> Todas los valores descriptivos o categoricos en las observaciones ya vienen convertidos a numericos en el dataset original. Aqui un detalle de los mismos:

**Reason for absence**

        1. Certain infectious and parasitic diseases
        2. II Neoplasms
        3. Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism
        4. Endocrine, nutritional and metabolic diseases
        5. Mental and behavioural disorders
        6. Diseases of the nervous system
        7. Diseases of the eye and adnexa
        8. Diseases of the ear and mastoid process
        9. Diseases of the circulatory system
        10. Diseases of the respiratory system
        11. Diseases of the digestive system
        12. Diseases of the skin and subcutaneous tissue
        13. Diseases of the musculoskeletal system and connective tissue
        14. Diseases of the genitourinary system
        15. Pregnancy, childbirth and the puerperium
        16. Certain conditions originating in the perinatal period
        17. Congenital malformations, deformations and chromosomal abnormalities
        18. Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified
        19. Injury, poisoning and certain other consequences of external causes
        20. External causes of morbidity and mortality
        21. Factors influencing health status and contact with health services.
        And 7 categories without (CID): 
          patient follow-up (22), 
          medical consultation (23), 
          blood donation (24), 
          laboratory examination (25), 
          unjustified absence (26), 
          physiotherapy (27), 
          dental consultation (28).		  
**Day of the week**

    (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))
**Seasons**

    (summer (1), autumn (2), winter (3), spring (4))
**education**

    (high school (1), graduate (2), postgraduate (3), master and doctor (4))
**Disciplinary failure**

    (yes=1; no=0)
**Social drinker**

    (yes=1; no=0)
**Social smoker**

    (yes=1; no=0)


Dado que el archivo es un csv y solo tiene 740 observaciones, primero haremos una rapida inspeccion manual con un notepad++. Como hemos dicho anteriormente posee todos valores numericos, tenemos encabezado de columnas y todos los valores de cada fila estan separados por punto y coma. Todos los numeros son enteros a excepcion al parecer del work load average. Mientras que por ej la estatura y el peso estan expresados en cm y kilos respectivamente.

Dicho eso comenzaremos con el analisis exploratorio del dataset. 

## Limpieza y exploracion de los datos (EDA)


Primero que nada importamos todas las librerias que estaremos usando o preveemos utilizar.


```{r}
# importing files library
if(!require(readr)){
    install.packages('readr', repos='http://cran.us.r-project.org')
    library(readr)
}
# manipulating data library
if(!require(dplyr)){
    install.packages('dplyr', repos='http://cran.us.r-project.org')
    library(dplyr)
}
# visualizing data library
if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}
# for plotting and validatin/g clusters
if(!require(factoextra)){
    install.packages('factoextra', repos='http://cran.us.r-project.org')
    library(factoextra)
}
# Arranging multiple grid-based plots on a page
if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}
# Clustering  library
if(!require(cluster)){
    install.packages('cluster', repos='http://cran.us.r-project.org')
    library(cluster)
}
# Clustering  library
if(!require(fpc)){
    install.packages('fpc', repos='http://cran.us.r-project.org')
    library(fpc)
}
# Reshaping data for ggplot
if(!require(reshape2)){
    install.packages('reshape2', repos='http://cran.us.r-project.org')
    library(reshape2)
}
# for PCA
if(!require(stats)){
    install.packages('stats', repos='http://cran.us.r-project.org')
    library(stats)
}


# Leemos el archivo csv delimitado por ;
abs_df <- read_delim("Absenteeism_at_work.csv", col_names = TRUE, delim=';')

```



### Chequeo y limpieza de datos

Hacemos las primeros chequeos del contenido del dataset:
```{r}
# visualizamos las primeras 5 observaciones
head(abs_df,5)

# Estadisticas basicas 
summary(abs_df)

# Verificamos la estructura y contenido del conjunto de datos
str(abs_df)
```


```{r}

# Dado que los nombres de las columnas tienes espacios, los quitamos agregando puntos:
names(abs_df)<-make.names(names(abs_df),unique = TRUE)

# Chequeamos ahora los nuevos nombres de columnas
colnames(abs_df)

```


Si bien, segun la descripcion del dataset en UCI y con la inspeccion visual no hay missing values, realicemos un chequeo rapido:

```{r}
# Estadísticas de valores vacíos
colSums(is.na(abs_df))
# y ahora los missing
colSums(abs_df=="")
```

Ahora analicemos algunas variables numerica y visualmente para entender mejor los datos:

```{r}
# Veamos los valores distintos de cada atributo
sapply(abs_df, function(x) length(unique(x)))
```
Aqui vemos que tenemos 13 meses informados de ausencia, arranquemos analizando esa variable con graficas simples.

Probemos tambien dos mas, la educacion de los empleados con la cantidad de horas de ausentimos


```{r}

a = ggplot(abs_df,aes(x=Month.of.absence,fill=Month.of.absence))+geom_bar(fill="lightblue")+theme_bw()

b = ggplot(abs_df,aes(x=Education,fill=Education))+geom_bar(fill="lightblue")+theme_bw()

c = ggplot(abs_df,aes(x=Absenteeism.time.in.hours,fill=Absenteeism.time.in.hours))+geom_bar(fill="lightblue")+theme_bw()

grid.arrange(a, b, nrow = 1, ncol = 2)

grid.arrange(c, nrow = 1)

# vemos que los datos para ese mes no informado, son pocos, solo 3 obs.
table(abs_df$Month.of.absence)

# Chequeemos esos datos:
select(filter(abs_df, Month.of.absence ==0), ID,Reason.for.absence,Day.of.the.week, Education, Absenteeism.time.in.hours)
```
En cuanto a los datos con mes no informado, luego podriamos eliminar estos registros ya que como vemos:

1 tienen razon de ausentismo en 0 y ademas la cantidad de horas informadas es cero, eso indica que son personas que nunca han faltado y por eso no hay ninguna razon de ausentismo???

2 son datos del nivel de educacion 1 que corresponde al que mayor cantidad de registros tenemos, por lo que quitar 3 filas de el no deberia representar un sesgo.

Mientras que vemos en la grafica que para la variable education predominan las de valor 1. Pero claro si bien tener las variables numericas nos permite utilizar varias funciones y aplicar algoritmos, para un analisis exploratorio inicial seria bueno tener los valores descriptivos reales para entender "mas facil" los datos.

Asi que iremos agregando variables al dataset con las descripciones de estas variables numericas. 

  (high school (1), graduate (2), postgraduate (3), master and doctor (4))
  
Antes de comenzar a agregar columnas descriptivas, chequemos el punto 1 que acabamos de comentar.
  
```{r}
# Validamos para los reason 0, si hay horas de ausencia
table(filter(abs_df, Reason.for.absence ==0)$Absenteeism.time.in.hours)

# Validamos para los que no tienen horas informadas que tengan 0 en reason
table(filter(abs_df, Absenteeism.time.in.hours == 0)$Reason.for.absence)
```

Bueno, al parecer el reason 0 se corresponde a los que no se han ausentando al trabajo, salvo solo 1 obversacion donde tiene un reason de ausencia pero no ha informado horas. Por lo no afectara a nuestros analisis estos registros.
Con esto tambien revalidamos que necesitamos tener disponibles los valores descriptivos para este tipo de analisis exploratorio que estamos haciendo ya que sino  tendriamos que ir a buscar cada vez a que descripcion corresponde cada ID de variable.


Pero antes de comenzar a sacar conclusiones sobres los datos y empezar a cruzar variables podriamos discretizar variables como:

* Age
* Distance.from.Residence.to.Work
* Body.mass.index
* Absenteeism.time.in.hours

Ya que para age ya hemos visto que tenemos bastante dispersos las edades y mejor agruparlas por los clasicos rangos de edad. Con la distancia nos paso algo similar.

Para Body mass index al ser un indice nos dice poco sin contrastarlo con algo que conozcamos asi que mejor agruparlo segun los criterios que se los suele analizar a nivel medico:

    < 18.5 Underweight
    18.5-25 Normal weight
    25-30 Overweight
    > 30  Obese

Mientras que las horas de ausentismo vimos en la grafica que la mayor cantidad de empleados faltan pocas horas y mejor armar grupos para el analisis:
    
    0 h horas
    1-3 horas
    4-8 horas
    9-16 horas
    17-40 horas 
    + 40 horas


### Creacion de Variables y Discretizacion

```{r}
# Creamos Seasons Desc
abs_df <- abs_df %>% mutate(Reason.for.absence.Desc = case_when
                   (Reason.for.absence == 0 ~ 'No Aplica',
                    Reason.for.absence == 1 ~ 'Infectious',
                    Reason.for.absence == 2 ~ 'Neoplasms',
                    Reason.for.absence == 3 ~ 'Immune System & Blood Issues',
                    Reason.for.absence == 4 ~ 'Metabolic diseases',
                    Reason.for.absence == 5 ~ 'Mental & Behavior disorders',
                    Reason.for.absence == 6 ~ 'nervous system diseases',
                    Reason.for.absence == 7 ~ 'eye and adnexa diseases',
                    Reason.for.absence == 8 ~ 'ear and mastoid diseases',
                    Reason.for.absence == 9 ~ 'circulatory diseases',
                    Reason.for.absence == 10 ~ 'respiratory diseases',
                    Reason.for.absence == 11 ~ 'digestive diseases',
                    Reason.for.absence == 12 ~ 'skin diseases',
                    Reason.for.absence == 13 ~ 'musculoskeletal diseases',
                    Reason.for.absence == 14 ~ 'genitourinary diseases',
                    Reason.for.absence == 15 ~ 'Pregnancy, and related',
                    Reason.for.absence == 16 ~ 'perinatal conditions',
                    Reason.for.absence == 17 ~ 'Congenital malformations',
                    Reason.for.absence == 18 ~ 'abnormal clinical findings',
                    Reason.for.absence == 19 ~ 'Injury, poisoning related',
                    Reason.for.absence == 20 ~ 'morbidity and mortality',
                    Reason.for.absence == 21 ~ 'Other Factors',
                    Reason.for.absence == 22 ~ 'patient follow-up',
                    Reason.for.absence == 23 ~ 'medical consultation',
                    Reason.for.absence == 24 ~ 'blood donation',
                    Reason.for.absence == 25 ~ 'laboratory examination',
                    Reason.for.absence == 26 ~ 'unjustified absence',
                    Reason.for.absence == 27 ~ 'physiotherapy',
                    Reason.for.absence == 28 ~ 'dental consultation'
                    )
                  )


# Creamos Seasons Desc
abs_df <- abs_df %>% mutate(Seasons.Desc = case_when
                  (Seasons == 1 ~ 'Summer',
                   Seasons == 2 ~ 'Autumn',
                   Seasons == 3 ~ 'Winter',
                   Seasons == 4 ~ 'Spring')
                  )

# Creamos Education Desc
abs_df <- abs_df %>% mutate(Education.Desc = case_when
                  (Education == 1 ~ 'HighSchool',
                   Education == 2 ~ 'Graduate',
                   Education == 3 ~ 'PostGraduate',
                   Education == 4 ~ 'Ms & Dr')
                  )

# Creamos Day of the week Desc
abs_df <- abs_df %>% mutate(Day.of.the.week.Desc = case_when (
                   Day.of.the.week == 2 ~ 'Monday',
                   Day.of.the.week == 3 ~ 'Tuesday',
                   Day.of.the.week == 4 ~ 'Wednesday',
                   Day.of.the.week == 5 ~ 'Thursday',                    
                   Day.of.the.week == 6 ~ 'Friday'
                    ))

# Creamos age range
abs_df["age_range"] <- as.factor(cut(abs_df$Age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c(1, 2, 3, 4,5,6,7,8)))
# Donde se corresponden a los siguientes rangos respectivamente:
# c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79")


# Creamos Distance range
abs_df["distance_range"] <- as.factor(cut(abs_df$Distance.from.Residence.to.Work, breaks = c(0,10,20,30,40,100), labels = c(1, 2, 3, 4, 5)))
# Donde se corresponden a los siguientes rangos respectivamente:
# c("10km", "20km", "30km", "40km", "+40km")


# Creamos BMI
abs_df["BMI"] <- as.factor(cut(abs_df$Body.mass.index, breaks = c(0,18.5,25,30,100), labels = c(1, 2, 3, 4)))
# Donde se corresponden a los siguientes rangos respectivamente:
# c("Underweight", "Normal", "Overweight", "Obese")

# Creamos absenteeism_range
abs_df <- abs_df %>% mutate(absenteeism_range = case_when
                   (Absenteeism.time.in.hours == 0 ~ '0 h',
                    between(Absenteeism.time.in.hours, 1, 3) ~ '1-3 h',
                    between(Absenteeism.time.in.hours, 4, 8) ~ '4-8 h',
                    between(Absenteeism.time.in.hours, 9, 16) ~  '9-16 h',
                    between(Absenteeism.time.in.hours, 17, 40) ~ '17-40 h',
                    Absenteeism.time.in.hours > 40 ~ '+ 40 h'
                   )
                )


# Mientras que variables como Disciplinary failure, Social drinker y Social smoker no haran falta convertirlas a descriptivas ya que es claro que 1 es Yes y 0 es No. Lo mismo sucede con Day of the week, es simple determinar de que estamos hablando cuando vemos los valores numericos. Caso similar con month, todos sabemos que mes representan los valores del 1 al 12, siendo el 0 lo que comentamos antes, valores no informados que no afectaran al modelo, aunque haremos nuestros testeos mas adelante en la practica.


```

Dicho eso y luego de haber agregado los campos descriptivos y discretizado sigamos analizando el resto de las variables:

### Exploracion de datos




```{r}

pet = ggplot(abs_df,aes(x=Pet,fill=Pet))+geom_bar(fill="lightblue")+theme_bw()
Seasons = ggplot(abs_df,aes(x=Seasons.Desc,fill=Seasons.Desc))+geom_bar(fill="lightblue")+theme_bw()

age = ggplot(abs_df,aes(x=Age,fill=Age))+geom_bar(fill="lightblue")+theme_bw()
day = ggplot(abs_df,aes(x=Day.of.the.week,fill=Day.of.the.week))+geom_bar(fill="lightblue")+theme_bw()

disciplinary = ggplot(abs_df,aes(x=Disciplinary.failure,fill=Disciplinary.failure))+geom_bar(fill="lightblue")+theme_bw()
distance = ggplot(abs_df,aes(x=Distance.from.Residence.to.Work,fill=Distance.from.Residence.to.Work))+geom_bar(fill="lightblue")+theme_bw()

drinker = ggplot(abs_df,aes(x=Social.drinker,fill=Social.drinker))+geom_bar(fill="lightblue")+theme_bw()
smoker = ggplot(abs_df,aes(x=Social.smoker,fill=Social.smoker))+geom_bar(fill="lightblue")+theme_bw()


weight = ggplot(abs_df,aes(x=Weight,fill=Weight))+geom_bar(fill="lightblue")+theme_bw()
mass = ggplot(abs_df,aes(x=Body.mass.index,fill=Body.mass.index))+geom_bar(fill="lightblue")+theme_bw()

grid.arrange(pet,Seasons, age, day, disciplinary, distance, drinker, smoker, weight, mass, nrow = 5, ncol = 2)

#library(forcats)
reason = ggplot(abs_df) +
    geom_bar(aes(x = forcats::fct_infreq(Reason.for.absence.Desc)
                 #,fill="Education"
                 ), fill="lightblue")+theme_bw()+theme(axis.text.x=element_text(size=8,angle=90))

grid.arrange(reason, nrow = 1)


absenteeism = ggplot(abs_df)+geom_bar(aes(x=forcats::fct_infreq(as.factor(Absenteeism.time.in.hours) )),fill="lightblue")+theme_bw()

grid.arrange(absenteeism, nrow = 1)

```

Vemos ahora como quedaron distribuidos los datos de estas variables post discretizacion


```{r}

# Comparacion age y age discretizada
ant = ggplot(abs_df,aes(x=Age,fill=Age))+geom_bar(fill="lightblue")+xlab("Age Before")+theme_bw()
desp = ggplot(abs_df,aes(x=age_range,fill=age_range))+geom_bar(fill="lightblue")+xlab("Age After")+theme_bw()
grid.arrange(ant,desp, nrow = 1, ncol = 2)

# Distancia y Distancia discretizada
ant = ggplot(abs_df,aes(x=Distance.from.Residence.to.Work,fill=Distance.from.Residence.to.Work))+geom_bar(fill="lightblue")+xlab("Distance Before")+theme_bw()
desp = ggplot(abs_df,aes(x=distance_range,fill=distance_range))+geom_bar(fill="lightblue")+xlab("Distance After")+theme_bw()
grid.arrange(ant,desp, nrow = 1, ncol = 2)

# BMI y BMI discretizada
ant = ggplot(abs_df,aes(x=Body.mass.index,fill=Body.mass.index))+geom_bar(fill="lightblue")+xlab("BMI Before")+theme_bw()
desp = ggplot(abs_df,aes(x=BMI,fill=BMI))+geom_bar(fill="lightblue")+xlab("BMI After")+theme_bw()
grid.arrange(ant,desp, nrow = 1, ncol = 2)


# Horas de Ausentimos discretizada
ant = ggplot(abs_df)+geom_bar(aes(x=forcats::fct_infreq(as.factor(Absenteeism.time.in.hours) )),fill="lightblue")+xlab("Absenteeism Before")+theme_bw()

desp = ggplot(abs_df)+geom_bar(aes(x=forcats::fct_infreq(as.factor(absenteeism_range) )),fill="lightblue")+xlab("Absenteeism After")+theme_bw() 
 
grid.arrange(ant,desp, nrow = 1, ncol = 2)
```

Comencemos ahora a contrastar variables, encontrar correlaciones, etc.. en definitiva entender aun mejor los datos.


* Education y Ausentismo
```{r}
# Segun el nivel educativo no cambia mucho el % de horas de ausentismo es parejo en todos los niveles, el mayor porcentaje ronda entre unas horas y un dia entero. No mucho mas que eso.
ggplot(abs_df,aes(x=Education.Desc,fill=absenteeism_range))+geom_bar(position="fill")+xlab("Absenteeism Absoluto")

# Pero ahora, si es muy evidente que la mayor cantidad de personas que se ausentan son las que alcanzaron solo el High School, en el resto de los niveles el ausentismo se reduce notablemente.
ggplot(abs_df,aes(x=absenteeism_range,fill=Education.Desc))+geom_bar(position="fill")+xlab("Absenteeism Fill")

```

Esto no implica que la gente con menor educacion falte mucho o no, pero al menos en los datos que tenemos es un punto a considerar y tenerlo presente. Pero si quizas tenga que ver con la edad tal vez? veamos...


* Age y Ausentismo
```{r}
#Ahora analicemos la edad, el ausentismo y como dijimos antess combinemoslo con la educacion, en particular para los de high school.

# En general en "porcentaje" se ausentan mas veces los de 20 años, pero si vemos a que medida que aumenta la edad, aumenta tambien la cantidad de horas que se ausentan los empleados, se ve esa minima tendencia aqui:
ggplot(abs_df,aes(x=age_range,fill=absenteeism_range))+geom_bar(position="fill")

#Y por lo que terminamos viendo aqui, los que mas se ausentan en realidad son los del rango de los 30 años.
ggplot(abs_df,aes(x=absenteeism_range,fill=age_range))+geom_bar(position="fill")

# Y con este ultima grafica pareciera que los mas faltan son los que llegaron hasta el high school, y son mayores de 30 años.
ggplot(abs_df,aes(x=absenteeism_range,fill=Education.Desc))+geom_bar(position="fill")+facet_wrap(~age_range)

# pero no podemos decir lo mismo si lo vemos de esta forma, ya que en porcentajes tanto graduados como tambien postgraduados tienen altos porcentajes de ausentismo incluso en el rango de los 20 años.
ggplot(abs_df,aes(x=absenteeism_range,fill=age_range))+geom_bar(position="fill")+facet_wrap(~Education.Desc)
```

* Smokers & Drinkers

```{r}
# Con estas dos comparaciones podemos ver que el fumador tiene menos peso que el bebedor, ya que un fumador puede fumar o no, podemos decir que hay un 50 y 50. Pero es claro viendolo desde el punto de vista de los bebedores sociales: hay muy pocos fumadores, sea cual sea su condicion. Eso es bueno mas alla del dataset, un vicio menos!

smoker = ggplot(abs_df,aes(x=as.factor(Social.smoker),fill=as.factor(Social.drinker)))+geom_bar(position="fill")+facet_wrap(~absenteeism_range)
drinker = ggplot(abs_df,aes(x=as.factor(Social.drinker),fill=as.factor(Social.smoker)))+geom_bar(position="fill")+facet_wrap(~absenteeism_range)

smoker
drinker

# Digamos que tienen mas tendencia a faltar los bebedores sociales y no asi los fumadores.

```


Pero que pasa ahora con el indice de masa corporal y la distancia al trabajo?

* BMI y Ausentismo

```{r}
# en valores absolutos parece no haber una gran diferencia entre las personas de peso normal vs los empleados con sobrepeso u obesos
abs = ggplot(abs_df,aes(x=absenteeism_range,fill=BMI))+geom_bar()+theme(axis.text.x=element_text(size=8,angle=90))+xlab("Valores Absolutos")

# Pero si lo vemos en porcentajes, parece que en general los empleados con peso normal faltaran mas, por lo que tener sobrepeso no es algo que afecte al presentimo.
per = ggplot(abs_df,aes(x=absenteeism_range,fill=BMI))+geom_bar(position="fill")+xlab("Porcentajes")+theme(axis.text.x=element_text(size=8,angle=90))

grid.arrange(abs,per, nrow = 1, ncol = 2)
```

* Distancia y Ausentismo

```{r}
g1 = ggplot(abs_df,aes(x=absenteeism_range,fill=(distance_range)))+geom_bar(position="fill")+xlab("Porcentajes")+theme(axis.text.x=element_text(size=8,angle=90))

g2 = ggplot(abs_df,aes(x=absenteeism_range,fill=(distance_range)))+geom_bar()+xlab("Valores Absolutos")+theme(axis.text.x=element_text(size=8,angle=90))

# Con los rangos de distancia que armamos no parece haber una gran diferencia o algo que nos indique que rango falta mas o menos, pero si hubieramos agrupado en 2 rangos entre menos de 20km vs mas de 20km, ahi veriamos claramente una diferencia. Hay una gran diferencia entre ambas rangos de distancia. Cuanto mas lejos vivan parece que hubiera una tendencia a que se ausenten mas.
grid.arrange(g1,g2, nrow = 1, ncol = 2)
```

Podriamos seguir combinando variables, e incluso comenzar a usar una variable mas que importante: Reason.for.absence, para contrastarla con las horas y demas variables, pero dado que tenemos muchas variables podemos usar a partir de aqui graficos de correlacion y heatmaps para ver las correlaciones mas rapidamente.

```{r}
# Una de las herramientas más útiles es calcular la matriz de correlación entre las variables. Con la función qplot y la correlación de variables, calculada con la función cor, podemos visualizar de manera fácil aquellas variables más correlacionadas, que corresponden a una intensidad mayor de color.
heat <- abs_df[,2:21]
  
qplot(x=Var1, y=Var2, data=melt(cor(heat, use="p")), fill=value, geom="tile") +
   theme(axis.text.x = element_text(angle = 90)) +
   coord_fixed()
```


```{r}
# El heatmap nos permite ir un paso más allá y agrupar las variables que tienen más relación entre ellas, a partir de un algoritmo de clustering a partir de la información de la correlación
abs_matrix <- as.matrix(scale(cor(abs_df[,2:21], use="p")))
heatmap(abs_matrix, Colv=F, scale='none')
```

Mientras que este otro grafico tambien nos permite encontrar variables correlacionadas y removerlas:

```{r}
data_corr<- cor(abs_df[,2:21])
corrplot::corrplot(data_corr, order = "hclust", tl.cex = 0.6, addrect = 8)
```


Aqui nos encontramos con algo que no habiamos analizado anteriormente de manera manual, como por ej, variables como: service time, hit target, disciplinary failure, todas ellas muestran correlacion ademas de las que comentamos antes, como obvias como podian ser education, BMI, Social Drinker, Distancia, etc...


Como vimos hasta ahora hemos hecho varios analisis de variables, y tal vez para acotar mejor el analisis y determinar mas facil clusters u obtener reglas o patrones mas sencillos debamos acotar el nro de variables aplicando tecnicas de reduccion dimensionalidad. 

En este caso usaremos PCA, tecnica que nos permite fusionar o crear nuevos atributos a partir de los existentes. A grandes rasgos, PCA es una transformación lineal de las variables. Cada nueva variable de un registro se contruye a partir de la antiguas variables de ese mismo registro a través de una transformación lineal fija.

Veamoslo en detalle.

### Reduccion de Dimensionalidad: PCA

Si bien a nivel macro los pasos para aplicar PCA son:

Estandarizar variables

Calcular la matriz de covarianzas MC

Generar los eigenvectores y eigenvalores a partir de la matriz MC

Ordenar los eigenvectores a partir de los eigenvalores de manera descendente y quedarnos con los TOP k eigenvectores

Contruir la matriz de projecciones MP, a partir de los eigenvectores seleccionados

Transformar el dataset original a partir de la matriz MP para obtener las nuevas dimensiones.

Nos apoyaremos en las funciones existentes en R que permiten calcular directamente las componentes principales y los principal component scores de cada observación sin tener que ir paso por paso.

Comencemos calculando la varianza de las variables del dataset:


```{r}
sort(apply(X = abs_df[,2:20], MARGIN = 2, FUN = var), decreasing =TRUE)
```


Como vemos tenemos valores muy extremos de varianzas en el orden de los 4500 para los gastos de viaje hasta llegar a valores en el orden de los 0.05 para los fallos disciplinarios. Deberemos estandarizar las variables para que tengan media cero y desviación estándar 1 antes de realizar el estudio PCA.


```{r}
set.seed(123)

# Con esta funcion prcomp podemos estandarizar las variables y hacer que la desviacion estandar sea 1 con el parametro scale=TRUE
pca <- prcomp(abs_df[,2:20], scale = TRUE)
names(pca)



# Donde la variable center contiene la media de cada variable antes de estandarizar
sort(pca$center, decreasing =TRUE)


# Mientras que rotation contiene el valor de los loadings ϕ para cada componente (eigenvector). El número de componentes principales se corresponde con el mínimo(n-1,p), que en este caso es  min(740,19)=19
pca_rot = pca$rotation

# veamos algunas registros:
head(pca_rot,5)

# Ahora si las varianzas estan mas igualadas post estandarizacion:
apply(X = pca_rot, MARGIN = 2, FUN = var)


```

Entender el vector de loadings que forma cada componente nos puede ayudar a interpretar que clase de información capta cada componente, por ejemplo si miramos la componente 1:

```{r}
# lo que vemos es que la primer componente capta mas informacion de la variable Education, cantidad de mascotas y gastos de viaje positivamente y negativamente del BMI, Weight y Service Time.
sort(pca_rot[,'PC1'], decreasing =TRUE)
# y asi podriamos seguir analizando el resto de las componentes para entender que variables tienen mas pesos sobre otras.


# Otra de las variables que genera la funcion prcomp es la matriz x, la cual es el resultado de multiplicar los datos por los loadings
head(pca$x)

# 740 valores x 19 componentes
dim(pca$x)
```


Veamos como se ven en un grafico de dos dimesiones al menos las primeras dos componentes:

```{r}
biplot(x = pca, scale = 0, cex = 0.5, col = c("lightblue", "blue"))

#fviz_pca_var(pca,col.var='blue')
```


Ya obtenidas las componentes principales, se puede saber cual es la varianza explicada por cada una de ellas, la proporción respecto a la varianza total y la proporción acumulada de la varianza.


```{r}

# Proporcion de la varianza explicada
fviz_eig(pca,addlabels=T)

# Proporcion acumulada de la varianza explicada
proporcion_varianza <- pca$sdev^2 / sum(pca$sdev^2)
proporcion_varianza_acum <- cumsum(proporcion_varianza)
proporcion_varianza_acum
ggplot(data = data.frame(proporcion_varianza_acum, pc = 1:19),
       aes(x = pc, y = proporcion_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(x = "Componentes principales",
       y = "Proporcion acumulada de la varianza explicada ")

```


**Dicho todo esto, si quisieramos por ejemplo explicar al menos un minimo de 70% de la varianza deberiamos usar las primeras 8 componentes principales.** o 10 si quisieramos explicar el 80%. El objetivo siempre es buscar aquellas componentes que explican la maxima varianza, esto es porque, queremos retener la mayor cantidad de informacion posible usando estas componentes. Entonces, cuanto mayor es la varianza explicada, mayor sera la informacion contenida en estas componentes.

Veamos o determinemos para proximos analisis cuales son las variables mas relevantes en las primeras dos componentes, ya que cuando contamos con muchas variables, podríamos decidir mostrar solo aquellas con mayor contribución.

```{r}
# Calidad de presentación de variables en un correlograma.
fviz_cos2(pca,choice='var',axes=1:2)


# Contribución de las variables a los respectivos componentes principales
fviz_contrib(pca,choice='var',axes=1, top = 10) #componente 1
fviz_contrib(pca,choice='var',axes=2, top = 10) #componente 2
```


La línea roja discontinua indica el valor medio de contribución. Para una determinada componente, una variable con una contribución mayor a este límite puede considerarse importante a la hora de contribuir a esta componente. En las representaciones anteriores, la variable Body.Mass.Index es la que más contribuye a la PC1.

### Analisis No Supervisado 

Antes de comenzar a contruir un modelo usando o no usando el resultado generado por PCA, realizaremos mas analisis no supervisado para investigar caracteristicas de posibles clusters y simplificar aun mas el dataset.

#### Determinacion de Clusters

Para determinar posibles valores de k clusters, primero usaremos metodos de validacion como Silueta Media, Elbow y Calinski-Harabasz. Realizaremos esto previo a generar los clusters ya que los métodos de particionamiento, como el agrupamiento en clústeres k-means, PAM o Hierarchical clustering requieren que se especifiquen el número de clústeres que se generarán.


La silueta media evalúa como de bien o mal está clasificada una muestra en el cluster al que ha sido asignada. Para ello se usa una fórmula que tiene en cuenta la distancia a las muestras de su clúster y la distancia a las muestras del clúster vecino más cercano. 

Mientras que Elbow para evaluar cual es el mejor número de clústers se considera el mejor modelo, aquel que ofrece la menor suma de los cuadrados de las distancias de los puntos de cada grupo con respecto a su centro, con la mayor separación entre centros de grupos. Es una idea conceptualmente similar a la silueta. En definitiva Elbow no es más que la selección del número de clústers en base a la inspección de la gráfica que se obtiene al iterar con el mismo conjunto de datos para distintos valores del número de clústers. Se seleccionará el valor que se encuentra en el “codo” de la curva.

Para facilitar la busqueda el K mas acorde a nuestros datos usando los metodos anteriores haremos uso de las funciones fviz_nbclust y kmeansruns que nos permiten simular distintos valores de K y determinar cual es el mas optimo.


La funcion fviz_nbclust es una forma sencilla de estimar el número K óptimo de clusters cuando no se dispone de información adicional en la que basarse, además de permitirnos representar los resultados.

La funcion kmeansrun del paquete fpc ejecuta internamente el algoritmo kmeans con un conjunto de valores, para después seleccionar el valor del número de clústers que mejor funcione.


```{r}
set.seed(123)

#Primero nos quedamos solo con las variables numericas
abs_df_reducido = abs_df[,c(2:21)]

# chequearemos entonces con las siguientes variables que por ejemplo vimos parecian determinantes al momento de realizar EDA.
abs_df_clustering_check1 <- select(abs_df_reducido, Absenteeism.time.in.hours,Weight,Age, Body.mass.index, Service.time)

# Ahora validemos la calidad de posibles clusters con 3 tipos posibles algoritmos: pam, hcut y kmeansruns y a la vez con 3 tipos de metodos: elbow (wss), silhouette(asw) y CAlinsky-Harabasz (ch)
#------------------------------------------------------------------------------
# check1: pam & elbow
fviz_nbclust(abs_df_clustering_check1, FUNcluster = pam, method = "wss") 

# check2: pam & silhouette
fviz_nbclust(abs_df_clustering_check1, FUNcluster = pam, method = "silhouette") 

#------------------------------------------------------------------------------
# check1: Hierarchical Clustering & elbow
fviz_nbclust(abs_df_clustering_check1, FUNcluster = hcut, method = "wss") 

# check2: Hierarchical Clustering & silhouette
fviz_nbclust(abs_df_clustering_check1, FUNcluster = hcut, method = "silhouette") 
#------------------------------------------------------------------------------

# check3: kmeansruns & CAlinsky-Harabasz/silhouette
fit_ch  <- kmeansruns(abs_df_clustering_check1, krange = 1:15, criterion = "ch") 
fit_asw <- kmeansruns(abs_df_clustering_check1, krange = 1:15, criterion = "asw")

fit_ch$bestk
fit_asw$bestk
#------------------------------------------------------------------------------
```


Vemos que en general podemos llegar a tener entre 2 y 4 clusters.

Hagamos un chequeo similar ahora con las primeras 4 variables en orden decreciente del vector de loadings que forman parte de la componente principal PC1, para validar que cantidad de clusters encontramos y poder asi certificar lo encontrado antes


```{r}
set.seed(123)

#primeras 4 Variables de la componente principal PC1, orden creciente
abs_df_clustering_check2 = select(abs_df_reducido, Absenteeism.time.in.hours, Education, Pet, Transportation.expense, Social.smoker)


# check1: pam & elbow
fviz_nbclust(abs_df_clustering_check2, FUNcluster = pam, method = "wss") 

# check2: pam & silhouette
fviz_nbclust(abs_df_clustering_check2, FUNcluster = pam, method = "silhouette") 



# check1: Hierarchical Clustering & elbow
fviz_nbclust(abs_df_clustering_check2, FUNcluster = hcut, method = "wss") 

# check2: Hierarchical Clustering & silhouette
fviz_nbclust(abs_df_clustering_check2, FUNcluster = hcut, method = "silhouette") 


# check3: kmeansruns & CAlinsky-Harabasz/silhouette
fit_ch  <- kmeansruns(abs_df_clustering_check2, krange = 1:15, criterion = "ch") 
fit_asw <- kmeansruns(abs_df_clustering_check2, krange = 1:15, criterion = "asw")
fit_ch$bestk
fit_asw$bestk
```


En este caso con estas variables en principio con PAM encontramos un rango de cluster similar a lo que vimos antes, como tambien con hcut+elbow, salvo para hcut+silutea media donde obtuvimos  una posible cantidad de 7 clusters, mientras que con kmeansruns nos dio una cantidad desorbitante de clusters entre 13 y 14.

Dicho esto considero que un valor mas real de cantidad de cluster para analizar los datos podria llegar a ser k=3. Incluso pensemos como fuimos agrupando durante el EDA los rangos de horas de ausencia, donde en general podriamos haber agrupado las horas en no mas de 5 posibles rangos.

Grafiquemos entonces los clusters con la funcion fviz_cluster con los metodos pam  como con hcut para analizarlos de manera mas visual para el set de variables correspondientes al primer grupo que vimos abs_df_clustering_check1.

El metodo pam, corresponde PAM clustering particiona los datos en k clusters alrededor de los "medoids", el cual puede ser definido como el objeto de un grupo cuya disimilaridad media a todos los objetos en el grupo es mínima. Es el punto ubicado más hacia el centro en todo el grupo. Es más robusto ante el ruido y a partes aisladas que el clasico k-means porque minimiza una suma de disimilaridades (entre pares de puntos) en vez de una suma de distancias euclidianas cuadradas.

Mientras que el metodo hcut en R, se corresponde a lo que denominamos como Hierarchical Clustering, o Agrupamiento Jerarquico. Que es un método de análisis de grupos puntuales, el cual busca construir una jerarquía de grupos. 

Veamos con mas detalle cada metodo a continuacion.

#### PAM-Partitioning around Medoids
 
K-Medoids (PAM-Partitioning around Medoids) es un método de clustering muy similar a K-means en cuanto a que ambos agrupan las observaciones en K clusters, donde K es un valor preestablecido por el analista. La diferencia es que, en K-medoids, cada cluster está representado por una observación presente en el cluster (medoid), mientras que en K-means cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular.

Una definición más exacta del término medoid es: elemento dentro de un cluster cuya distancia (diferencia) promedio entre él y todos los demás elementos del mismo cluster es lo menor posible.

##### Pasos del Algoritmo PAM:

1. Seleccionar K observaciones aleatorias como medoids iniciales. También es posible identificarlas de forma específica.

2. Calcular la matriz de distancia entre todas las observaciones si esta no se ha calculado anteriormente.

3. Asignar cada observación a su medoid más cercano.

4. Para cada uno de los clusters creados, comprobar si seleccionando otra observación como medoid se consigue reducir la distancia promedio del cluster, si esto ocurre, seleccionar la observación que consigue una mayor reducción como nuevo medoid.

5. Si al menos un medoid ha cambiado en el paso 4, volver al paso 3, de lo contrario, se termina el proceso.

Pasemos a ver como se comporta este algoritmo.


```{r}
set.seed(123)

# PAM Cluster with euclidean metric
abs_df_pam_euclidean <- pam(x = abs_df_clustering_check1, k = 3, metric = "euclidean")

fviz_cluster(abs_df_pam_euclidean, data  = abs_df_clustering_check1 ,
                    geom = "point",
             ellipse.type = "convex", 
            show.clust.cent = TRUE,
            shape =   10,
             ggtheme = theme_bw())

# PAM Cluster with manhattan metric
abs_df_pam_manhattan<- pam(x = abs_df_clustering_check1, k = 3,  metric = "manhattan")

fviz_cluster(abs_df_pam_manhattan, data  = abs_df_clustering_check1 ,
                    geom = "point",
             ellipse.type = "convex", 
            show.clust.cent = TRUE,
            shape =   10,
             ggtheme = theme_bw())
```
en cada cluster se ven outliers, seguramente haya que depurar un poco mas la informacion y/o analizar mas en profundidad estos casos o reveer la cantidad de clusters

Usamos el parametro metric para usar dos metodos, euclidean y manhattan, para calcular las distancias **around medoids**, de hecho la descripcion de este parametro es la siguiente.

El parametro **metric** es una cadena de caracteres que especifica la métrica que se utilizará para calcular las diferencias entre las observaciones.
Las opciones disponibles actualmente son "euclidean" y "manhattan". Las distancias euclidianas son la raíz de la suma de cuadrados de las diferencias, y las distancias de Manhattan son la suma de las diferencias absolutas. Si x ya es una matriz de disimilitud, este argumento se ignorara.

Pero si miramos mas la documentacon de PAM vemos que tenemos otro parametro:
**stand** que es un parametro logico. Si se asigna TRUE, las medidas en x se estandarizan antes de calcular las diferencias. Las mediciones se estandarizan para cada variable, restando el valor medio de la variable y dividiendo por la desviación absoluta media de la variable. Si x ya es una matriz de disimilitud, este argumento se ignorará.

Probemos nuevamente PAM aplicando este parametro tal vez estandarizando las variables logramos hacer "mas normales" cada una de ellas y lograr mejores clusters. Veamoslo:



```{r}
set.seed(123)
# PAM Cluster with euclidean metric
abs_df_pam_euclidean_stand <- pam(x = abs_df_clustering_check1, k = 4, stand=TRUE, metric = "euclidean")

fviz_cluster(abs_df_pam_euclidean_stand, data  = abs_df_clustering_check1 ,
                    geom = "point",
             ellipse=TRUE,
             ellipse.type = "convex",
             show.clust.cent = TRUE,
             shape =   10,
             ggtheme = theme_bw())

# PAM Cluster with manhattan metric
abs_df_pam_manhattan_stand<- pam(x = abs_df_clustering_check1, k = 3,  stand=TRUE, metric = "manhattan")

fviz_cluster(abs_df_pam_manhattan_stand, data  = abs_df_clustering_check1 ,
                    geom = "point",
             ellipse=TRUE,
             ellipse.type = "convex",
            show.clust.cent = TRUE,
            shape =   10,
             ggtheme = theme_bw())
```


En estas 4 ejecucion que hemos hecho de PAM con las diferentes metricas, visualmente vemos mas definidos los clusters cuando se estandarizo las variables. Por lo que nos quedaremos con estas dos ultimas variables para el posterior conclusion final.


#### Agglomerative Hierarchical Clustering


En esta sección vamos a realizar el proceso de clusterizado utilizando Hierarchical Clustering. El algoritmo funciona de la siguiente forma:

##### Pasos del Algoritmo:

1. Cada registro del dataset empieza generando su propio cluster, por lo que nos encontramos con tantos clusteres como elementos queramos procesar.

2. Se aplica una función para comprobar cual es la semejanda y afinidad entre los diferentes clusteres (se suele usar la distancia euclidea). Aquellos que se consideran parecidos son fusionados.

3. Se sigue aplicando el algoritmo hasta que se llega al número de clusteres indicado.

Pasemos a ver como se comporta este algoritmo.


```{r}
set.seed(123)

# HClustering  with euclidean metric
abs_df_hcut_euclidean_k3=hcut(x=abs_df_clustering_check1,k=3,hc_method="complete",hc_metric='euclidean')


fviz_cluster(abs_df_hcut_euclidean_k3, data=abs_df_clustering_check1 ,
              geom = "point",
              ellipse.type = "convex", 
              show.clust.cent = TRUE,
              shape =   10,
              ggtheme = theme_bw())


# HClustering with manhattan metric
abs_df_hcut_manhattan_k3=hcut(x=abs_df_clustering_check1,k=3,hc_method="complete",hc_metric='manhattan')

fviz_cluster(abs_df_hcut_manhattan_k3, data=abs_df_clustering_check1 ,
              geom = "point",
              ellipse.type = "convex", 
              show.clust.cent = TRUE,
              shape =   10,
              ggtheme = theme_bw())

#---------------------------------------

# HClustering  with euclidean metric
abs_df_hcut_euclidean_stand_k3=hcut(x=abs_df_clustering_check1,k=3,hc_method="complete",hc_metric='euclidean',stand=TRUE)


fviz_cluster(abs_df_hcut_euclidean_stand_k3, data=abs_df_clustering_check1 ,
              geom = "point",
              ellipse.type = "convex", 
              show.clust.cent = TRUE,
              shape =   10,
              ggtheme = theme_bw())


# HClustering with manhattan metric
abs_df_hcut_manhattan_stand_k3=hcut(x=abs_df_clustering_check1,k=3,hc_method="complete",hc_metric='manhattan',stand=TRUE)

fviz_cluster(abs_df_hcut_manhattan_stand_k3, data=abs_df_clustering_check1 ,
              geom = "point",
              ellipse.type = "convex", 
              show.clust.cent = TRUE,
              shape =   10,
              ggtheme = theme_bw())



#---------------------------------------

# HClustering  with euclidean metric
abs_df_hcut_euclidean_stand_k4=hcut(x=abs_df_clustering_check1,k=4,hc_method="complete",hc_metric='euclidean',stand=TRUE)


fviz_cluster(abs_df_hcut_euclidean_stand_k4, data=abs_df_clustering_check1 ,
              geom = "point",
              ellipse.type = "convex", 
              show.clust.cent = TRUE,
              shape =   10,
              ggtheme = theme_bw())


# HClustering with manhattan metric
abs_df_hcut_manhattan_stand_k4=hcut(x=abs_df_clustering_check1,k=4,hc_method="complete",hc_metric='manhattan',stand=TRUE)

fviz_cluster(abs_df_hcut_manhattan_stand_k4, data=abs_df_clustering_check1 ,
              geom = "point",
              ellipse.type = "convex", 
              show.clust.cent = TRUE,
              shape =   10,
              ggtheme = theme_bw())


```

En este caso luego de aplicar Hcut a nuestro set de variables vemos que los clusters queda muy bien definidos, incluso bastante mejor que con PAM, ya que no hay solapamiento de cluster. Ademas hemos logrado mejores resultados en este caso con la metrica manhattan. Tanto con 3 clusters sean estandarizados los datos o no. De hecho con manhattan hasta hemos obtenido un cuarto cluster super definido con un pequeño grupo de observaicionees de la muestra al estandarizar variables.


### Conclusiones

Luego de haber aplicado tanto pam, como hcut, con diferentes valores de K, con diferentes metricas (euclidean y manhattan) y estandarizando o no las variables, vemos que con k=3 se definen bien los clusters. Aunque si debemos decir que con hcut los clusters estan mucho mas marcados que con pam, y hasta incluso podemos generar 4 clusters bien independientes.

Para analizar el resultado de haber aplicado estos algoritmos no supervisados y entender las caracteristicas de cada cluster para cada uno de los metodos aplicados podemos haer uso de la funcion aggragate como vemos aqui:



```{r}

# Para abs_df_pam_euclidean_stand con k=4
aggregate(abs_df_clustering_check1,by=list(abs_df_pam_euclidean_stand$cluster),mean)

# Para abs_df_pam_manhattan_stand con k=3
aggregate(abs_df_clustering_check1,by=list(abs_df_pam_manhattan_stand$cluster),mean)
```

Vemos que para el primer grupo de clusters con PAM, tenemos los 3 primero con caracteristicas bastantes similares ya para el rango de edad comprendido entre los 29 y 39 años, y con un BMI entre los 23 y 30, cuya mayoria esta formada por gente con sobrepeso, la cantidad de horas de ausencia rondaria de 4 a 5 horas. Mientras que el 4to cluster es bastante distintivo ya que agrupa el personal que se ausentaria mas de 50hs pero con caracteristicas simialres a los otros 3 cluster por lo que es bastante extraña esta agrupacion.

El 2do grupo de clusters con PAM, tiene la particularidad haber separado en clusters de 5 y 8 horas de ausentismo, la diferencia esta dada por la diferencia de edad mayores y menores de 30 años y BMI que indican peso normal o sobrepeso.

Veamos ahora lo que obtuvimos con Hcut y validemos si los clusters visualmente mejor definidos se traducen en caracteristicas mas distintivas y claras:

```{r}
# Para abs_df_hcut_manhattan_k3
aggregate(abs_df_clustering_check1,by=list(abs_df_hcut_manhattan_k3$cluster),mean)

# Para abs_df_hcut_manhattan_stand_k3
aggregate(abs_df_clustering_check1,by=list(abs_df_hcut_manhattan_stand_k3$cluster),mean)

# Para abs_df_hcut_manhattan_stand_k4
aggregate(abs_df_clustering_check1,by=list(abs_df_hcut_manhattan_stand_k4$cluster),mean)

```

Como vemos en los 3 grupos de clusters obtenidos con hcut, las caracteristicas de cada clusters dentro de cada grupo son bien distintivas. Vemos para los 3 grupos que en general se obtienen rangos tiempos de ausentismo menores a 4 horas, de 4 a 8, de mas de 1 semana, y hasta 2 semanas de ausencias, teniendo en cuenta horas laborables de jornadas de 8hs. Y en general vemos que los que menos se ausentan tienen un valor elevado de service time, estan en su adultez y con un BMI que indica obesidad. En contrapartida con los clusteres que se ausentan mas que tienen casi un BMI que comienza a decrece y un service time cercano a la mediana.


Dicho todo esto pasemos a la construccion de un modelo que nos permita predecir todo esto que fuimos analizando hasta ahora a partir de la exploracion de los datos.

## Construccion del modelo

Para construir un modelo que nos permite predecir las horas de ausentismo deberemos primero que nada generar dos set de datos uno para efectivamente construir y entrenar el modelo y un segudo set de datos para validar ese mismo modelo.

### Generacion de dataset de training y testing

Con la función sample_frac de dplyr generamos una muestra aleatoria de nuestro dataset, donde crearemos el dataset de train especificando la proporción del dataset original que queremos, y finalmente asignaremos al dataset de test el resto de las filas que no han estado seleccionadas con la función anti_join. Esta division es necesaria para la posterior validacion de nuestro modelo. Es por eso que necesitamos tener dos conjuntos, el de entrenamiento y el de prueba. 


En primer instancia construyamos un modelo a partir de todas la variables disponibles en el dataset original previo a haber agregado variables descriptivas, haber discretizado o aplicado PCA, y chequemos el resultado.

Es por eso que arranquemos generando los dos set de datos para ese primer objetivo:

```{r}
set.seed(123)
#colnames(abs_df) 

rows <- 1:nrow(abs_df)
abs_df <- abs_df %>% mutate(rowID = rows)
                            
train <- sample_frac(abs_df[rows,], .75)
test  <-   anti_join(abs_df[rows,], train, by='rowID') 

# Datasets de entrenamiento
training_x <- train[,c(2:20)]
training_y <- train$absenteeism_range

# Datasets de validacion
testing_x <- test[,c(2:20)]
testing_y <- test$absenteeism_range
```



### Creacion del modelo y extraccion de reglas

Se crea el árbol de decisión usando los datos de entrenamiento.

```{r}
set.seed(123)
training_y = as.factor(training_y)
#training_x = as.factor(training_x)
abs_model <- C50::C5.0(training_x, training_y, rules=TRUE )


print(abs_model)
summary(abs_model)

```

Vemos que al usar el dataset original donde tenemos la variable **Razon de ausencia** muchas reglas con esta variables son obvias e introducen ruido en las posibles reglas o mejor dicho c5.0 la variable mas important para comenzar la clasificacion. Decimos que nos agrega ruido, ya que sabemos que si hay una razon es claro q habra una cantidad de horas de ausencia,... pero que pasa si queremos predecir en base al comportamiento o caracteristicas del empleado si faltara y en que rango de horas, no tiene sentido usar esa variable porque seguramente la cantidad de horas de ausencia puede estar relacionada casi directamente al tipo de motivo medico de la consulta medica o padecimiento del empleado. Aunque claro podriamos tambien hacer foco en este punto y segun el padecimiento o tipo de consulta medica sabriamos que faltara cierta cantidad de tiempo. O incluso podriamos hacer foco, en el padecimiento y entender un poco que reglas se pueden obtener o incluso solo analizar el reason "consulta medica" que parece bastante generico y con ese filtrado obtener reglas, etc... hay varias opciones de analizar.
Pero en este caso queremos determinar u obtener reglas mas alla de la consulta, sino mas bien focaliar en las caracteristicas de la persona y comportamiento.

Asi que repitamos el proceso pero sin la razon de ausencia.

```{r}
set.seed(123)

abs_df <- abs_df %>% filter(!Month.of.absence==0)
rows <- 1:nrow(abs_df)
abs_df <- abs_df %>% mutate(rowID = rows)
                            
train <- sample_frac(abs_df[rows,], .75)
test  <-   anti_join(abs_df[rows,], train, by='rowID') 


# Datasets de entrenamiento
training_x <- train[,c(3:20)]
training_y <- train$absenteeism_range

# Datasets de validacion
testing_x <-   test[,c(3:20)]
testing_y <-   test$absenteeism_range

training_y = as.factor(training_y)
abs_model <- C50::C5.0(training_x , training_y, rules=TRUE )

summary(abs_model)
print(abs_model)

```

Antes de describir las reglas pintemos el arbol y veamos que tal se ve.
```{r}
set.seed(20)
training_y = as.factor(training_y)
abs_model <- C50::C5.0(training_x, training_y )


plot(abs_model)

# dada la cantidad de nodos y ramas, es mejor analizarlo filtrando los niveles o subrarboles. En casos asi es incluso es mas claro ver directamente las reglas o usar otro tipo de grafica mas que un simple plot.
plot(abs_model, subtree = 25)
```


Las reglas que obtuvimos a partir de nuestros datos fueron 18, describamos aqui las primeras 5 con mayor lift:

Rule 1: (30, lift 16.3)
	Disciplinary.failure > 0
	->  class 0 h  [0.969]
	
Al parecer los que posee un fallo disciplinario no se ausentan? esta es la primer regla?.. En un proceso de dataming real claramente volveriamos atras o aqui mismo deberiamos analizar estos datos. Pero bueno si el modelo esta funcionando a partir del dataset lo podemos pensar como que el que tuvo un fallo disciplinario y fue alertado por eso, no quiera o no tenga intencion de ausentar por temor a algun otro tipo de represalia.	

Rule 2: (8, lift 1.9)
	Month.of.absence <= 6
	Seasons <= 1
	Social.drinker <= 0
	->  class 1-3 h  

Rule 3: (5, lift 1.8)
	Month.of.absence > 8
	Seasons <= 1
	Social.drinker <= 0
	->  class 1-3 h  [0.857]

Con las dos reglas de arriba podemos determinar que los no fumadores independiente del momento del año puden llegar a aunsentarse hasta 3 horas.

Rule 4: (59/13, lift 1.6)
	Month.of.absence <= 2
	Day.of.the.week > 2
	Age <= 38
	->  class 1-3 h  [0.770]

Rule 5: (39/9, lift 1.6)
	Month.of.absence <= 10
	Education > 2
	Weight <= 80
	->  class 1-3 h  [0.756]
	
El mismo rango de horas suelen ausentarse las personas menores de 38 años con un peso menor a 80kg, en cualquier momento del año.



## Evaluacion del modelo

Si bien las primeras 5 reglas no nos dicen mucho veamos esta poca eficiencia en numeros evaluando el modelo. En principio lo haremos con el modelo tal como lo construimos en el step anterior, con los datos originales. Osea sin aplicar PCA.

### sin PCA

Validemos ahora el modelo obtenido en el paso anterior:

```{r}

predicted_model <- predict(abs_model, testing_x , type="class")

# Donde la precision promedio del modelo es:
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testing_y) / length(predicted_model)))


```
Analicemos los verdaderos positivos y verdaderos negativos, y la sensibilidad a partir de las matrices de confusion:

```{r}
library(gmodels)
CrossTable(testing_y, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))


```



### con PCA

Pero que pasaria si ahora contruimos un modelo sobre nuestros datos pero luego de haber aplicado PCA.

Como ya hemos calculado todas las componentes principales, y las tenemos en el objeto "pca" podemos acceder a la componenente 1 de la siguiente forma, por lo que utilizaremos esta variables para contruir un modelo mediante arboles de decision, entrenarlo y luego validarlo.

```{r}
set.seed(123)

# Agregamos la variable objetivo de nuevo a los datos rotados generados por PCA anteriormente
pca <- prcomp(abs_df[,2:20], scale = TRUE)
pca_and_target_data=data.frame(absenteeism_range=abs_df$absenteeism_range, pca$x)

# nos quedamos con la variable objetivo y las primeras 10 componentes ppales que representan el 80% de la varianza de los datos.
pca_PC10 = pca_and_target_data[,1:9] 

#--------------------------------------------------------

rows <- 1:nrow(pca_PC10)
pca_PC10 <- pca_PC10 %>% mutate(rowID = rows)
                            
train <- sample_frac(pca_PC10[rows,], .75)
test  <-   anti_join(pca_PC10[rows,], train, by='rowID') 


# Nos quedamos con la variables mas representativas para contruir el modelo y obtener reglas
# Datasets de entrenamiento
training_x <- train[,c(2:9)]
training_y <- train$absenteeism_range

# Datasets de validacion
testing_x <-   test[,c(2:9)]
testing_y <-   test$absenteeism_range

training_y = as.factor(training_y)
abs_pca_model <- C50::C5.0(training_x , training_y )


print(abs_pca_model)
summary(abs_pca_model)

predicted_pca_model <- predict(abs_pca_model, testing_x , type="class")

# Donde la precision promedio del modelo es:
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_pca_model == testing_y) / length(predicted_pca_model)))

```



### Conclusiones:Capacidad Predictiva c/PCA vs s/PCA

En nuestro caso al aplicar arboles de decision post aplicacion PCA obtuvimos una mejora casi nula en la precision del arbol, otra alternativa aqui podria ser aplicar alguna tecnica de poda, ya sea con o sin PCA para lograr una mayor eficiencia.

Dado que nuestro dataset es muy pequeño se pueden seguir probando otros algoritmos para ver la posibilidad si conseguimos una mejor precision, ya sea con o sin PCA. O mismo se podria cambiar la estrategia de haber armado rangos horarios a predecir, ya que podriamos haber considerado las horas de ausencia como una variabel dependiente continua y haber aplicado regresion lineal. Son varias las alternativas segun la estrategia seleccionada. Ademas de que no siempre, creo yo, no hay que enfocarse en unico algoritmo predictivo, mas a un como en este caso donde el costo computacional de ejecutar n algoritmos es nulo dado el tamaño del dataset. Asi que podriamos volver a etapas anteriores de nuestro mini proyecto de datamining y arrancar con otras estrategias y aplicando los mismos u otras tecnicas.

## Integracion: Responda al objetivo

Mas alla de que nuestro modelo, tal como lo hemos analizado y con la estrategia elegida, tiene solo una precision de 63% es igualmente util para extraer algunos insights que nos permitan  establecer reglas de negocio en un sistema **X**.

Por ejemplo en la plataforma Microstrategy, se podrian generar dashboards con una metrica que posean embebido nuestro modelo R y por ende nos vaya graficando como performa la metrica a partir de la carga diaria de datos al datawarehouse corporativo y recalcularse para permitir a los analistas ver la evolucion del modelo.

Esta primer iteracion permitiria entonces dar un primer paso para entender los datos y la performance del modelo, y luego si con mas "indicios o pistas" poder realizar una segunda iteracion del proyecto y focalizar en una estrategia o algoritmo particular.

******
# Rúbrica
******
* 10%. Trata todos los puntos de la práctica como un proyecto real de minería de datos, siguiendo todos los puntos del ciclo de vida, desde la descripción funcional hasta la integración de los resultados, comentando los puntos en los que sería necesario volver atrás cuando sea necesario.
* 15%. Se generan reglas y se comentan e interpretan las más significativas. Adicionalmente se genera matriz de confusión para medir la capacidad predictiva del algoritmo.  
* 15%. Se genera modelo no supervisado, se muestran y comentan medidas de calidad del modelo generado y se comentan las conclusiones.  
* 15%. Se genera modelo no supervisado con métrica de distancia distinta al anterior. Se muestran y comentan medidas de calidad del modelo generado y se comentan las conclusiones. Adicionalmente se comparan los dos modelos no supervisados con métricas de distancia distinta.  
* 15%. Se genera un modelo supervisado sin PCA/SVD previo, se muestran y comentan medidas de calidad del modelo generado y se comenta extensamente el conocimiento extraído del modelo.  
* 15%. Se genera un modelo supervisado con PCA/SVD previo, se muestran y comentan medidas de calidad del modelo generado y se comenta extensamente el conocimiento extraído del modelo.    
* 15%. Se compara la capacidad predictiva de los dos modelos supervisados y se comenta la diferencia de rendimiento en base al efecto PCA/SVD.  


******
# Recursos de programación
******
* Incluimos en este apartado una lista de recursos de programación para minería de datos donde podréis encontrar ejemplos, ideas e inspiración:
  + [Material adicional del libro: Minería de datos Modelos y Algoritmos](http://oer.uoc.edu/libroMD/)
  + [Espacio de recursos UOC para ciencia de datos](http://datascience.recursos.uoc.edu/es/)
  + [Buscador de código R](https://rseek.org/)  
  + [Colección de cheatsheets en R](https://rstudio.com/resources/cheatsheets/)  
  

******

******






