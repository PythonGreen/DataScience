---
title: 'Minería de datos: PEC3 - Clasificación con árboles de decisión'
author: "Autor: Pablo A. Delgado"
date: "Diciembre 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 4
    includes:
      in_header: 75.584-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=T, echo=T)
```


******
# Introducción
******
## Presentación
Esta prueba de evaluación continua cubre los Módulos 3 (Clasificación:
árboles de decisión) y el Módulo 8 (Evaluación de modelos) del programa de la asignatura.

## Competencias
Las competencias que se trabajan en esta prueba son:  

* Uso y aplicación de las TIC en el ámbito académico y profesional.
* Capacidad para innovar y generar nuevas ideas.
* Capacidad para evaluar soluciones tecnológicas y elaborar propuestas de proyectos teniendo en cuenta los recursos, las alternativas disponibles y las condiciones de mercado.
* Conocer las tecnologías de comunicaciones actuales y emergentes así como saberlas aplicar convenientemente para diseñar y desarrollar soluciones basadas en sistemas y tecnologías de la información.
* Aplicación de las técnicas específicas de ingeniería del software en las diferentes etapas del ciclo de vida de un proyecto.
* Capacidad para aplicar las técnicas específicas de tratamiento, almacenamiento y administración de datos.
* Capacidad para proponer y evaluar diferentes alternativas tecnológicas para resolver un problema concreto.

## Objetivos
La correcta asimilación del Módulo 3. En esta PEC trabajaremos la generación e interpretación de un árbol de decisión con el software de prácticas. Seguiremos también con la preparación de los datos y la extracción inicial de conocimiento.

## Descripción de la PEC a realizar
La prueba está estructurada en un total de un único ejercicio práctico.

## Recursos Básicos
**Material docente proporcionado por la UOC.** 

Módulo 3 y 8 del material didáctico.

**Complementarios** 

* Los descritos para la anterior PEC.
* Fichero titanic.csv
* R package C5.0 (Decision Trees and Rule-Based Models): https://cran.r-project.org/web/packages/C50/index.html


## Criterios de valoración

Todos los ejercicios deben ser presentados de forma razonada y clara, especificando todos y cada uno de los pasos que se hayan llevado a cabo para su resolución. No se aceptará ninguna respuesta que no esté claramente justificada.

## Formato y fecha de entega
El formato de entrega es: usernameestudiant-PECn.html/doc/docx/odt/pdf.
Se recomienda la entrega en formato html y también el Rmd que genera el html entregado.
Fecha de Entrega: 18/12/2019.
Se debe entregar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en qué se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia  no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado  
******

En este ejercicio vamos a seguir los pasos del ciclo de vida de un proyecto de minería de datos, para el caso de un algoritmo de clasificación y más concretamente un árbol de decisión. Lo haremos con el archivo titanic.csv, que se encuentra adjunto en el aula. Este archivo contiene un registro por cada pasajero que viajaba en el Titanic. En las variables se caracteriza si era hombre o mujer, adulto o menor (niño), en qué categoría viajaba o si era miembro de la tripulación.

Objetivos:

*	Estudiar los datos, por ejemplo: ¿Número de registros del fichero? ¿Distribuciones de valores por variables? ¿Hay campos mal informados o vacíos?
*	Preparar los datos. En este caso ya están en el formato correcto y no es necesario discretizar ni generar atributos nuevos. Hay que elegir cuáles son las variables que se utilizarán para construir el modelo y cuál es la variable que clasifica. En este caso la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no.
*	Instalar, si es necesario, el paquete C5.0  Se trata de una implementación más moderna del algoritmo ID3 de Quinlan. Tiene los principios teóricos del ID3 más la poda automática. Con este paquete generar un modelo de minería.
*	¿Cuál es la calidad del modelo?
*	Generar el árbol gráfico.
* Generar y extraer las reglas del modelo.
*	En función del modelo, el árbol y las reglas: ¿Cuál es el conocimiento que obtenemos?
*	Probar el modelo generado presentándole nuevos registros. ¿Clasifica suficientemente bien?
  
##  Revisión de los datos, extracción visual de información y preparación de los datos

Carga de los datos:

```{r message= FALSE, warning=FALSE}
data<-read.csv("./titanic.csv",header=T,sep=",")
attach(data)
```


Empezaremos haciendo un breve análisis de los datos ya que nos interesa tener una idea general de los datos que disponemos. Por ello, primero calcularemos las dimensiones de nuestra base de datos y analizaremos qué tipos de atributos tenemos.

Para empezar, calculamos las dimensiones de la base de datos mediante la función dim(). Obtenemos que disponemos de 2201 registros o pasajeros (filas) y 4 variables (columnas). 

```{r}
dim(data)
```

¿Cuáles son esas variables? Gracias a la función str() sabemos que las cuatro variables son categóricas o discretas, es decir, toman valores en un conjunto finito. La variable CLASS hace referencia a la clase en la que viajaban los pasajeros (1ª, 2ª, 3ª o crew), AGE determina si era adulto o niño (Adulto o Menor), la variable SEX si era hombre o mujer (Hombre o Mujer) y la última variable (SURVIVED) informa si el pasajero murió o sobrevivió en el accidente (Muere o Sobrevive).

```{r}
str(data)
```

Es de gran interés saber si tenemos muchos valores nulos (campos vacíos) y la distribución de valores por variables. Es por ello recomendable empezar el análisis con una visión general de las variables. Mostraremos para cada atributo la cantidad de valores perdidos mediante la función summary.  

```{r}
summary(data)
```

Disponemos por tanto de un data frame formado por cuatro variables categóricas sin valores nulos. Para un conocimiento mayor sobre los datos, tenemos a nuestro alcance unas herramientas muy valiosas: las herramientas de visualización. Para dichas visualizaciones, haremos uso de los paquetes ggplot2, gridExtra y grid de R. 

```{r}
if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}
if(!require(grid)){
    install.packages('grid', repos='http://cran.us.r-project.org')
    library(grid)
}
if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}

```


Nos interesa describir la relación entre la supervivencia y cada uno de las variables mencionadas anteriormente. Para ello, por un lado graficaremos mediante diagramas de barras la cantidad de muertos y supervivientes según la clase en la que viajaban, la edad o el sexo. Por otro lado, para obtener los datos que estamos graficando utilizaremos el comando table para dos variables que nos proporciona una tabla de contingencia.

```{r}
grid.newpage()
plotbyClass <- ggplot(data,aes(CLASS,fill=SURVIVED))+geom_bar() +labs(x="Class", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Class")

plotbyAge <- ggplot(data,aes(AGE,fill=SURVIVED))+geom_bar() +labs(x="Age", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Age")

plotbySex <- ggplot(data,aes(SEX,fill=SURVIVED))+geom_bar() +labs(x="Sex", y="Passengers")+ guides(fill=guide_legend(title=""))+ scale_fill_manual(values=c("black","#008000"))+ggtitle("Survived by Sex")

grid.arrange(plotbyClass,plotbyAge,plotbySex,ncol=2)

```

De estos gráficos obtenemos información muy valiosa que complementamos con las tablas de contingencia (listadas abajo). Por un lado, la cantidad de pasajeros que sobrevivieron es similar en hombres y mujeres (hombres: 367 y mujeres 344). No, en cambio, si tenemos en cuenta el porcentaje respecto a su sexo. Es decir, pese a que la cantidad de mujeres y hombres que sobrevivieron es pareja, viajaban más hombres que mujeres (470 mujeres y 1731 hombres), por lo tanto, la tasa de muerte en hombres es muchísimo mayor (el 78,79% de los hombres murieron mientras que en mujeres ese porcentaje baja a 26,8%). 

En cuanto a la clase en la que viajaban, los pasajeros que viajaban en primera clase fueron los únicos que el porcentaje de supervivencia era mayor que el de mortalidad. El 62,46% de los viajeros de primera clase sobrevivió, el 41,4% de los que viajaban en segunda clase mientras que de los viajeros de tercera y de la tripulación solo sobrevivieron un 25,21% y 23,95% respectivamente. Para finalizar, destacamos que la presencia de pasajeros adultos era mucho mayor que la de los niños (2092 frente a 109) y que la tasa de supervivencia en niños fue mucho mayor (52,29% frente a 31,26%), no podemos obviar, en cambio, que los únicos niños que murieron fueron todos pasajeros de tercera clase (52 niños). 

```{r}
tabla_SST <- table(SEX, SURVIVED)
tabla_SST
prop.table(tabla_SST, margin = 1)
```

```{r}
tabla_SCT <- table(CLASS,SURVIVED)
tabla_SCT
prop.table(tabla_SCT, margin = 1)
```

```{r}
tabla_SAT <- table(AGE,SURVIVED)
tabla_SAT
prop.table(tabla_SAT, margin = 1) 
```

```{r}
tabla_SAT.byClass <- table(AGE,SURVIVED,CLASS)
tabla_SAT.byClass
```

Una alternativa interesante a las barras de diagramas, es el plot de las tablas de contingencia. Obtenemos la misma información pero para algunos receptores puede resultar más visual.  

```{r}
par(mfrow=c(2,2))
plot(tabla_SCT, col = c("black","#008000"), main = "SURVIVED vs. CLASS")
plot(tabla_SAT, col = c("black","#008000"), main = "SURVIVED vs. AGE")
plot(tabla_SST, col = c("black","#008000"), main = "SURVIVED vs. SEX")
```

Nuestro objetivo es crear un árbol de decisión que permita analizar qué tipo de pasajero del Titanic tenía probabilidades de sobrevivir o no. Por lo tanto, la variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no. De todas maneras, al imprimir las primeras (con head) y últimas 10 (con tail) filas nos damos cuenta de que los datos están ordenados.

```{r}
head(data,10)
tail(data,10)
```

Nos interesará "desordenarlos". Guardaremos los datos con el nuevo nombre como "data_random".

```{r}
set.seed(1)
data_random <- data[sample(nrow(data)),]
```

Para la futura evaluación del árbol de decisión, es necesario dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento es el subconjunto del conjunto original de datos utilizado para construir un primer modelo; y el conjunto de prueba, el subconjunto del conjunto original de datos utilizado para evaluar la calidad del modelo. 

Lo más correcto será utilizar un conjunto de datos diferente del que utilizamos para construir el árbol, es decir, un conjunto diferente del de entrenamiento. No hay ninguna proporción fijada con respecto al número relativo de componentes de cada subconjunto, pero la más utilizada acostumbra a ser 2/3 para el conjunto de entrenamiento y 1/3, para el conjunto de prueba. 

La variable por la que clasificaremos es el campo de si el pasajero sobrevivió o no, que está en la cuarta columna.

```{r}
set.seed(666)
y <- data_random[,4] 
X <- data_random[,1:3] 
```


Podemos elegir el subconjunto de entrenamiento y de prueba de diversas maneras. La primer opción consiste en calcular a cuántas filas corresponde dos tercios de los datos (2*2201/3=1467) y dividir "manualmente" el conjunto.

```{r}
trainX <- X[1:1467,]
trainy <- y[1:1467]
testX <- X[1468:2201,]
testy <- y[1468:2201]

```

En la segunda opción podemos crear directamente un rango.

```{r}
indexes = sample(1:nrow(data), size=floor((2/3)*nrow(data)))
trainX<-X[indexes,]
trainy<-y[indexes]
testX<-X[-indexes,]
testy<-y[-indexes]
```

Después de una extracción aleatoria de casos es altamente recomendable efectuar un análisis de datos mínimo para asegurarnos de no obtener clasificadores sesgados por los valores que contiene cada muestra. 

## Creación del modelo, calidad del modelo y extracción de reglas

Se crea el árbol de decisión usando los datos de entrenamiento (no hay que olvidar que la variable outcome es de tipo factor):

```{r}
trainy = as.factor(trainy)
model <- C50::C5.0(trainX, trainy,rules=TRUE )
summary(model)
```

Errors muestra el número y porcentaje de casos mal clasificados en el subconjunto de entrenamiento. El árbol obtenido clasifica erróneamente 304 de los 1467 casos dados, una tasa de error del 20.7%.

A partir del árbol de decisión de dos hojas que hemos modelado, se pueden extraer las siguientes reglas de decisión (gracias a rules=TRUE podemos imprimir las reglas directamente):

SEX = "Hombre" → Muere. Validez: 80,2%

CLASS = "3a" → Muere. Validez: 75.1%

CLASS "1ª", "2ª" o "Crew" y SEX = "Mujer" → Sobrevive. Validez: 90,5%

Por tanto podemos concluir que el conocimiento extraído y cruzado con el análisis visual se resume en "las mujeres y los niños primero a excepción de que fueras de 3ª clase".

A continuación mostramos el árbol obtenido.

```{r}
model <- C50::C5.0(trainX, trainy)
plot(model)
```


## Validación del modelo con los datos reservados
Una vez tenemos el modelo, podemos comprobar su calidad prediciendo la clase para los datos de prueba que nos hemos reservado al principio. 

```{r}
predicted_model <- predict( model, testX, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))
```

Cuando hay pocas clases, la calidad de la predicción se puede analizar mediante una matriz de confusión que identifica los tipos de errores cometidos. 

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```

Otra manera de calcular el porcentaje de registros correctamente clasificados usando la matriz de confusión:

```{r}

porcentaje_correct<-100 * sum(diag(mat_conf)) / sum(mat_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct))

```

Además, tenemos a nuestra disposición el paquete gmodels para obtener información más completa:

```{r}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}
```
```{r}
CrossTable(testy, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))
```


******
# Ejercicios
******

## Ejercicio 1:  
Partiendo del ejemplo mostrado, repetid el ejercicio con otro conjunto de datos. Pueden ser datos reales de vuestro ámbito laboral o de algún repositorio de datos de Internet. Mirad por ejemplo: http://www.ics.uci.edu/~mlearn/MLSummary.html i http://www.kaggle.com.

Es muy importante seleccionar correctamente el conjunto de datos y explicar de forma correcta la base de datos y la razón de su elección.

Podéis añadir o variar los puntos si lo consideráis necesario (por ejemplo, crear el modelo con todos los datos y validación cruzada, probar el boosting o variar el prunning ...) Recordad también que el ciclo de vida de los proyectos de minería contempla retroceder para volver a generar el modelo con datos modificados o parámetros del algoritmo variados si el resultado no es lo suficientemente bueno.

### Revision y preparacion de datos

Para dar continuacion a lo que se desarrollo en la PEC2 en el cual se analizo el dataset de semillas de trigo, y donde se hicieron los primeros analisis exploratorios como tambien se determinaron los clusters que mejor agregaban los datos, aqui usaremos los algoritmos de clasificacion para crear un modelo que nos permita predecir/clasificar nuevas datos de semillas.

**Dataset**: https://archive.ics.uci.edu/ml/datasets/seeds


Primero importemos los datos y repasemos que datos teniamos
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(data.table)
library(ggplot2)
library(C50)
library(dplyr)

temp_data <- readLines("https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt")

# como hicimos en la pec2, quitamos los TAB de mas
seeds_data <- fread(text=gsub("\t\t", "\t", temp_data), sep="\t")

# y le agregamos los nombres a las columnas
colnames(seeds_data) <- c("area", "perimeter", "compactness", "kernel_length","kernel_width","asymmetry_coeff","kernel_groove_length", "class")


```

Tenemos 210 observaciones y 8 variables, donde 1 de ellas es la clase que puede tomar el valor 1, 2 y 3 que se corresponden con estas 3 variedades de semilla: Kama, Rosa and Canadian.

```{r}
dim(seeds_data)
```
Veamos informacion de esas variables con str(). Como sabiamos, son todas numericas continuas, excepto class que es la variable clasificatoria, pudiendo tomar 3 valores.
```{r}
str(seeds_data)
```
Hagamos una descriptiva rápida de las variables del dataset
```{r}
summary(seeds_data)
```

Veamos como estan distribuidos las observaciones para esas 3 variedades:
```{r}
# Visualize frequency of diagnoses in dataset
ggplot(seeds_data,aes(class,fill=class))+geom_bar()+ggtitle("Freq class")
table(seeds_data$class)
```


```{r}

```
Pero claro el objetivo aqui es predecir la variedad de semilla de trigo a partir del dataset que tenemos. Para eso creemos un modelo basado en el algoritmo de arboles de decision C5.0:

### Creación del modelo, calidad del modelo y extracción de reglas

Con la función **sample_frac** de dplyr generamos una muestra aleatoria de nuestro dataset, donde crearemos el dataset de train especificando la proporción del dataset original que queremos, y finalmente asignaremos al dataset de test el resto de las filas que no han estado seleccionadas con la función **anti_join**. Esta division es necesaria para la posterior validacion de nuestro modelo. Es por eso que necesitamos tener dos conjuntos, el de entrenamiento y el de prueba. En este mismo documento previamente se explicaron dos tecnicas para splitear el dataset, pero como comentamos aprovecharemos la simplicidad que nos otorga usar funciones del paquete dplyr.

```{r}
set.seed(1)

rows <- 1:nrow(seeds_data)
train <- sample_frac(seeds_data[rows,], .75)
test  <- anti_join(seeds_data[rows,], train) #by=''

# Datasets de entrenamiento
trainx <- train[,1:7]
trainy <- train$class

# Datasets de validacion
testx <- test[,1:7]
testy <- test$class
```

Creemos ahora el modelo y analizemos info detallada del mismo

```{r}
trainy = as.factor(trainy)
model <- C50::C5.0(trainx, trainy,rules=TRUE )

summary(model)
```


Errors muestra el número y porcentaje de casos mal clasificados en el subconjunto de entrenamiento. El árbol obtenido clasifica erróneamente solo 4 de los 158 casos dados, una tasa de error del 2.5%.

A partir del árbol de decisión de dos hojas que hemos modelado, se pueden extraer 6 reglas de decisión que gracias al parametro "rules=TRUE" podemos imprimir las reglas directamente por pantalla como se ve mas arriba. Cada una con su porcentaje de precision, los cuales estan todos por encima del 91%.

Contrastando con lo obtenido en la PEC2, donde ya habiamos visto visualmente por como se separaban los clusters que el perimetro y el area eran buenas variables, es todo un hallazgo aqui que la variable en particular kernel_groove_length sea la primera en clasificarnos en dos los datos para luego si, con ayuda del perimetro seguir segmentando los datos para terminar clasificando correctamente las semillas.
Claro, el ojo humano pueda fallar a la hora de mirar un grafico y separar en 3 los datos visualmente como teniamos en esta matriz de graficos en la PEC2

![](matrizSeeds.jpg)


Ademas de que no es sencillo de ver en este grafico que primero clasificando por una de las variables, el resto de los datos se pueden segmentar aun mas por otra variable. Por eso la gran utilidad de este tipo de algoritmos y de haber hecho primero una EDA, generando clusters y demas para posteriormente aplicar clasificacion.




Veamos ahora un resumen del modelo generado y grafiquemos el arbol obtenido:
```{r}
model <- C50::C5.0(trainx, trainy)
print(model)
plot(model)
```

Bajamos un nivel para visualizar mas claramente las hojas del arbol.
```{r}
plot(model, subtree = 2)
```

### Validacion del Modelo

Primero apliquemos la funcion predict al conjunto de validacion a partir del modelo generado antes y luego mostremos el % de precision del modelo.
```{r}
predicted_model <- predict( model, testx, type="class" )

# Donde la precision promedio del modelo es:
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))
```

Y dado que tenemos solo 3 clases, podemos analizar la calidad de prediccion del modelo con una matriz de confusion, la cual identifica los tipos de errores cometidos:

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```

Y tal como se explico en el enunciado, se puede calcular tambien el porcentaje de registros correctamente clasificados a partir de la matriz de confusion:
```{r}
porcentaje_correct<-100 * sum(diag(mat_conf)) / sum(mat_conf)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct))
```
Que obviamente son el mismo valor.

Y por ultimo podemos hacer uso del paquete gmodels que nos da una matriz de confusion mas completa:
```{r}
library(gmodels)
CrossTable(testy, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))
```
Incluso se puede utilizar la funcion confusionMatrix de la libreria caret, que nos da mucho mejor informacion y de un solo vistazo:
```{r}
#install.packages("e1071")
library(caret)

confusionMatrix(data=as.factor(predicted_model),reference=as.factor(testy),positive="yes")
```
Como hemos dicho con los metodos anteriores, obtuvimos una exactitud del 92.3%, con un error del 7.7%. Esta ultima cifra es algo mayor que la obtenida al ajustar el modelo al dataset de training, lo cual era esperable. De cara a la implementacion, este modelo tiene una sensibilidad del 100%, 94% y 78% para la clase 1, clase 2 y clase 3 respectivamente. Es decir, el modelo es capaz de predecir en ese rango de porcentajes cada clase.


### Mejora del modelo

Los numeros de nuestro modelo son bastante buenos a partir del dataset que disponemos, pero imaginemos que queremos mejorar el modelo. Para ello, podemos hacer uso de las tecnicas de Boosting.

```{r}
modeloc50_trials <- C5.0(trainx, trainy, trials = 10) 
data_predicted_improved <- predict(modeloc50_trials, testx)
confusionMatrix (as.factor(data_predicted_improved),reference=as.factor(testy), positive = "yes")
```

La exactitud ha aumentado a un 96.15 % y la sensibilidad a disminuido a un 95% para clase 1, pero aumentado a un 100% y 92% para la clase 2 y 3 respectivamente. Se ha logrado una excelente mejora.




### Otros algoritmos de clasificacion basados en arboles de decision

Que obtendriamos si aplicamos otro algoritmo de arboles de decision a nuestro conjunto de training, en este caso usemos rpart. 

Veamos un resumen de su aplicacion:

```{r}
library(rpart)
library(rattle)

set.seed(1)
seeds_rpart = rpart(trainy~., data=trainx)
print(seeds_rpart)
```
Analicemos info mas detallada del arbol y grafiquemoslo:
```{r}
summary(seeds_rpart)

fancyRpartPlot(seeds_rpart, cex = 1.0, caption = "rattle::fancyRpartPlot (Wheat Seeds)")
```

Siendo la precision de este modelo sin aplicar ningun tipo de mejora o prunning la siguiente:
```{r}

rpart_predicted <- predict( seeds_rpart, testx, type="class" )
confMat <- table(testy,rpart_predicted)
accuracy <- sum(diag(confMat))/sum(confMat)

print(sprintf("La precisión del árbol es: %.4f %%",100*accuracy))


```


******
# Rúbrica
******
* 15% Se explica de forma clara la base de datos seleccionada y la razón de su elección.
* 10% Hay un estudio sobre los datos de los que se parte y los datos son preparados correctamente.
* 20% Se aplica un árbol de decisión de forma correcta y se obtiene una estimación del error.
* 5% Se muestra de forma gráfica el árbol obtenido.
* 10% Se explican las reglas que se obtienen.
* 10% Se usa el modelo para predecir con muestras no usadas en el entrenamiento y se obtiene una estimación del error.
* 15% Se prueba otro modelo de árbol o variantes diferentes del C50 obteniendo mejores resultados.	
* 5% Se presenta el código y es fácilmente reproducible.
* 10% Se presenta unas conclusiones donde se expone el conocimiento adquirido tras el trabajo realizado.

